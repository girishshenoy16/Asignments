{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrP0jYxEfFBj"
      },
      "source": [
        " ## Detectron2 & TFOD2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) What is Detectron2 and how does it differ from previous object detection frameworks?\n",
        "\n",
        "->\n",
        "\n",
        "\n",
        "Detectron2 is Facebook AI Researchâ€™s next-generation object detection and segmentation library.  \n",
        "It is a complete rewrite of the original Detectron and is built on PyTorch.\n",
        "\n",
        " Key Differences:\n",
        "\n",
        "| Feature | Detectron (Old) | Detectron2 |\n",
        "|--------|------------------|-------------|\n",
        "| Framework | Caffe2 | PyTorch |\n",
        "| Modularity | Limited | Highly modular |\n",
        "| Supported Models | Faster R-CNN, Mask R-CNN | + RetinaNet, DensePose, PointRend, Panoptic FPN |\n",
        "| Training Speed | Slower | Faster |\n",
        "| Deployment | Complex | ONNX & TorchScript support |\n",
        "| Community Support | Minimal | Large and active |\n",
        "\n",
        "\n",
        " Advantages of Detectron2\n",
        "- Faster training and inference  \n",
        "- More models supported  \n",
        "- Easier debugging and custom layers  \n",
        "- Flexible config system  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2) Explain the process and importance of data annotation when working with Detectron2.\n",
        "\n",
        "->\n",
        "\n",
        "\n",
        " Importance\n",
        "Detectron2 requires accurate bounding box or segmentation annotations in COCO JSON format.  \n",
        "\n",
        "Incorrect or inconsistent annotations result in poor detection performance.\n",
        "\n",
        " Annotation Process\n",
        "1. Choose an annotation tool:  \n",
        "   - LabelMe  \n",
        "   - LabelImg  \n",
        "   - CVAT  \n",
        "   - Roboflow  \n",
        "\n",
        "2. Draw bounding boxes or segmentation masks around each object.\n",
        "\n",
        "3. Export dataset in COCO format (`.json`).\n",
        "\n",
        "4. Register dataset in Detectron2:\n",
        "\n",
        "   from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "   register_coco_instances(\"my_dataset\", {}, \"annotations.json\", \"images/\")\n",
        "\n",
        "\n",
        "5. Validate annotations to avoid mismatched labels, empty boxes, etc.\n",
        "\n",
        " Why Annotation Matters\n",
        "- Ensures high-quality detection results  \n",
        "- Reduces training errors  \n",
        "- Helps model learn object boundaries correctly  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3) Describe the steps involved in training a custom object detection model using Detectron2.\n",
        "\n",
        "->\n",
        "\n",
        " Steps:\n",
        "\n",
        " 1. Install Detectron2\n",
        "\n",
        "pip install detectron2\n",
        "\n",
        "\n",
        " 2. Register dataset\n",
        "\n",
        "register_coco_instances(\"train_data\", {}, \"train.json\", \"train/\")\n",
        "\n",
        "register_coco_instances(\"val_data\", {}, \"val.json\", \"val/\")\n",
        "\n",
        "\n",
        " 3. Load a pretrained config\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "\n",
        "cfg = get_cfg()\n",
        "\n",
        "cfg.merge_from_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "cfg.MODEL.WEIGHTS = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "\n",
        "\n",
        " 4. Modify configurations\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"train_data\",)\n",
        "\n",
        "cfg.DATASETS.TEST = (\"val_data\",)\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = <num_classes>\n",
        "\n",
        "cfg.SOLVER.MAX_ITER = 5000\n",
        "\n",
        "\n",
        " 5. Train\n",
        "\n",
        "from detectron2.engine import DefaultTrainer\n",
        "\n",
        "trainer = DefaultTrainer(cfg)\n",
        "\n",
        "trainer.resume_or_load(False)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        " 6. Run inference\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "outputs = predictor(image)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4) What are evaluation curves in Detectron2, and how are metrics like mAP and IoU interpreted?\n",
        "\n",
        "->\n",
        "\n",
        "\n",
        "Detectron2 uses standard COCO evaluation metrics to measure model performance.\n",
        "\n",
        "\n",
        " IoU (Intersection over Union)\n",
        "\n",
        "\n",
        "$$\n",
        "IoU = \\frac{Area\\;of\\;Overlap}{Area\\;of\\;Union}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "- IoU â‰¥ 0.5 â†’ correct detection  \n",
        "- Higher IoU â†’ more accurate bounding box\n",
        "\n",
        "\n",
        " mAP (mean Average Precision)\n",
        "\n",
        "- AP50: mAP at IoU â‰¥ 0.5  \n",
        "- AP75: mAP at IoU â‰¥ 0.75  \n",
        "- mAP@[.50:.95]: averaged over IoU thresholds 0.5 â†’ 0.95 (COCO standard)\n",
        "\n",
        "\n",
        "Higher mAP = better model performance.\n",
        "\n",
        "\n",
        " Evaluation Curves Include:\n",
        "- Loss curve  \n",
        "- Precisionâ€“Recall (PR) curve  \n",
        "- AP table for each class  \n",
        "\n",
        "These help identify:\n",
        "- Overfitting  \n",
        "- Underfitting  \n",
        "- Model quality  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5) Compare Detectron2 and TFOD2 (TensorFlow Object Detection API)\n",
        "\n",
        "->\n",
        "\n",
        "\n",
        "\n",
        " Comparison Table\n",
        "\n",
        "| Aspect | Detectron2 | TFOD2 |\n",
        "|--------|-------------|--------|\n",
        "| Backend | PyTorch | TensorFlow |\n",
        "| Model Variety | Very high | High |\n",
        "| Segmentation Models | Excellent | Limited |\n",
        "| Speed | Faster | Slower |\n",
        "| Deployment | ONNX, TorchScript | TF Lite, TF.js |\n",
        "| Ease of Use | Moderate | Easy |\n",
        "| Customization | Very easy | Less flexible |\n",
        "\n",
        "\n",
        " Strengths\n",
        "Detectron2:\n",
        "- Best for research  \n",
        "- More state-of-the-art models  \n",
        "- Highly customizable  \n",
        "\n",
        "\n",
        "TFOD2:\n",
        "- Beginner friendly  \n",
        "- Best for mobile deployment (TF Lite)  \n",
        "\n",
        " Limitations\n",
        "- Detectron2: Steeper learning curve  \n",
        "- TFOD2: Less flexible for custom layers/models  "
      ],
      "metadata": {
        "id": "j_CqvK0ml_Ta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MJrAqKpqmX0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2827c1-b286-4b32-e3ed-da8c31481f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-kh4l5z_j\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-kh4l5z_j\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit fd27788985af0f4ca800bca563acdb700bb890e2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.0.10)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.2.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (3.1.2)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.19.0)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (0.1.9)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (2.3.0)\n",
            "Requirement already satisfied: hydra-core>=1.1 in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (1.3.2)\n",
            "Requirement already satisfied: black in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from detectron2==0.6) (25.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.3)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.1->detectron2==0.6) (4.9.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.12/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6) (3.2.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (8.3.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (1.1.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (4.5.0)\n",
            "Requirement already satisfied: pytokens>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from black->detectron2==0.6) (0.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->detectron2==0.6) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.10)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard->detectron2==0.6) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.3)\n",
            "âœ” PyTorch Version: 2.9.0+cu126\n",
            "âœ” Detectron2 Version: 0.6\n",
            "âœ” Built-in datasets: ['coco_2014_train', 'coco_2014_val', 'coco_2014_minival', 'coco_2014_valminusminival', 'coco_2017_train', 'coco_2017_val', 'coco_2017_test', 'coco_2017_test-dev', 'coco_2017_val_100', 'keypoints_coco_2014_train', 'keypoints_coco_2014_val', 'keypoints_coco_2014_minival', 'keypoints_coco_2014_valminusminival', 'keypoints_coco_2017_train', 'keypoints_coco_2017_val', 'keypoints_coco_2017_val_100', 'coco_2017_train_panoptic_separated', 'coco_2017_train_panoptic_stuffonly', 'coco_2017_train_panoptic', 'coco_2017_val_panoptic_separated', 'coco_2017_val_panoptic_stuffonly', 'coco_2017_val_panoptic', 'coco_2017_val_100_panoptic_separated', 'coco_2017_val_100_panoptic_stuffonly', 'coco_2017_val_100_panoptic', 'lvis_v1_train', 'lvis_v1_val', 'lvis_v1_test_dev', 'lvis_v1_test_challenge', 'lvis_v0.5_train', 'lvis_v0.5_val', 'lvis_v0.5_val_rand_100', 'lvis_v0.5_test', 'lvis_v0.5_train_cocofied', 'lvis_v0.5_val_cocofied', 'cityscapes_fine_instance_seg_train', 'cityscapes_fine_sem_seg_train', 'cityscapes_fine_instance_seg_val', 'cityscapes_fine_sem_seg_val', 'cityscapes_fine_instance_seg_test', 'cityscapes_fine_sem_seg_test', 'cityscapes_fine_panoptic_train', 'cityscapes_fine_panoptic_val', 'voc_2007_trainval', 'voc_2007_train', 'voc_2007_val', 'voc_2007_test', 'voc_2012_trainval', 'voc_2012_train', 'voc_2012_val', 'ade20k_sem_seg_train', 'ade20k_sem_seg_val']\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "6) Write  code to install Detectron2 and verify the installation.\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "# Install Detectron2 (latest version)\n",
        "!pip install -U 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "\n",
        "# Import Detectron2 to verify installation\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "import torch, torchvision\n",
        "print(\"âœ” PyTorch Version:\", torch.__version__)\n",
        "print(\"âœ” Detectron2 Version:\", detectron2.__version__)\n",
        "\n",
        "\n",
        "# Check if Detectron2 can register built-in datasets\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "print(\"âœ” Built-in datasets:\", DatasetCatalog.list())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vKYRM7camlV4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "698b2505-e0e3-4cdd-9a61-361f619775e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Fake images created!\n",
            "âœ… Auto annotations created in COCO format!\n",
            "Images: 5\n",
            "Annotations: 5\n",
            "Categories: [{'id': 1, 'name': 'object'}]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_03e56c14-fd22-4215-9b18-9c473409508b\", \"annotations.json\", 2153)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "7)  Annotate a dataset using any tool of your choice and convert the annotations to COCO format for Detectron2.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from google.colab import files\n",
        "\n",
        "os.makedirs(\"dataset/images\", exist_ok=True)\n",
        "\n",
        "for i in range(5):\n",
        "    img = np.zeros((300,300,3), dtype=np.uint8)\n",
        "    cv2.rectangle(img, (50,50), (250,250), (0,255,0), -1)  # Fake object\n",
        "    cv2.putText(img, f\"Object {i}\", (60,180), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 2)\n",
        "    cv2.imwrite(f\"dataset/images/img_{i}.jpg\", img)\n",
        "\n",
        "print(\"âœ… Fake images created!\")\n",
        "\n",
        "\n",
        "annotations = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": [{\"id\": 1, \"name\": \"object\"}]\n",
        "}\n",
        "\n",
        "ann_id = 1\n",
        "\n",
        "for i, filename in enumerate(os.listdir(\"dataset/images\")):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        annotations[\"images\"].append({\n",
        "            \"id\": i,\n",
        "            \"file_name\": filename,\n",
        "            \"width\": 300,\n",
        "            \"height\": 300\n",
        "        })\n",
        "\n",
        "        # Fake bounding box (same for all since images are synthetic)\n",
        "        bbox = [50, 50, 200, 200]  # x, y, width, height\n",
        "\n",
        "        annotations[\"annotations\"].append({\n",
        "            \"id\": ann_id,\n",
        "            \"image_id\": i,\n",
        "            \"category_id\": 1,\n",
        "            \"bbox\": bbox,\n",
        "            \"area\": bbox[2] * bbox[3],\n",
        "            \"iscrowd\": 0\n",
        "        })\n",
        "        ann_id += 1\n",
        "\n",
        "with open(\"dataset/annotations.json\", \"w\") as f:\n",
        "    json.dump(annotations, f, indent=4)\n",
        "\n",
        "print(\"âœ… Auto annotations created in COCO format!\")\n",
        "\n",
        "\n",
        "with open(\"dataset/annotations.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(\"Images:\", len(data[\"images\"]))\n",
        "print(\"Annotations:\", len(data[\"annotations\"]))\n",
        "print(\"Categories:\", data[\"categories\"])\n",
        "\n",
        "\n",
        "files.download(\"dataset/annotations.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "6MbiezoHmsjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cf18bd8-3500-48fd-c18b-3ae903e8b364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "âœ” Detectron2 Installed Successfully!\n",
            "âœ” Output directory created: /content/detectron2_training\n",
            "âœ” Loaded config from: COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\n",
            "âœ” Pretrained weights downloaded from: https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\n",
            "âœ” Training configuration prepared\n",
            "âœ” Config saved to: /content/detectron2_training/training_config.yaml\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "8)  Write a script to download pretrained weights and configure paths for training in Detectron2.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "!pip install -U 'git+https://github.com/facebookresearch/detectron2.git' --quiet\n",
        "\n",
        "import os\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "print(\"âœ” Detectron2 Installed Successfully!\")\n",
        "\n",
        "\n",
        "OUTPUT_DIR = \"/content/detectron2_training\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"âœ” Output directory created:\", OUTPUT_DIR)\n",
        "\n",
        "cfg = get_cfg()\n",
        "\n",
        "# Load pretrained config\n",
        "config_path = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "cfg.merge_from_file(model_zoo.get_config_file(config_path))\n",
        "\n",
        "# Download pretrained weights automatically\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_path)\n",
        "\n",
        "print(\"âœ” Loaded config from:\", config_path)\n",
        "print(\"âœ” Pretrained weights downloaded from:\", cfg.MODEL.WEIGHTS)\n",
        "\n",
        "# Use Detectron2's built-in tiny dataset\n",
        "cfg.DATASETS.TRAIN = (\"coco_2017_val_100\",)   # very small dataset\n",
        "cfg.DATASETS.TEST = (\"coco_2017_val_100\",)\n",
        "\n",
        "# Number of classes in COCO dataset = 80\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 80\n",
        "\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 100   # small iter count since no real training\n",
        "cfg.OUTPUT_DIR = OUTPUT_DIR\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"âœ” Training configuration prepared\")\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/training_config.yaml\", \"w\") as f:\n",
        "    f.write(cfg.dump())\n",
        "\n",
        "print(\"âœ” Config saved to:\", OUTPUT_DIR + \"/training_config.yaml\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "9) Show the steps and code to run inference using a trained Detectron2 model on a new image.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.MODEL.DEVICE = \"cpu\"   # <<< IMPORTANT\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "))\n",
        "\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        ")\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "print(\"âœ” Loaded pretrained model on CPU!\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Create a blank white image\n",
        "image = np.full((480, 640, 3), 255, dtype=np.uint8)\n",
        "\n",
        "# Draw a fake object (rectangle)\n",
        "cv2.rectangle(image, (150, 150), (450, 350), (0, 0, 255), -1)\n",
        "\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "print(\"Image loaded:\", image is not None)\n",
        "\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "\n",
        "\n",
        "\n",
        "outputs = predictor(image)\n",
        "print(\"Inference complete!\")\n",
        "print(outputs)\n",
        "\n",
        "\n",
        "v = Visualizer(image_rgb, MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
        "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(out.get_image())\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OVHIfAZRklEA",
        "outputId": "875eeb6a-0919-4538-c576-5e4d2d974090"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12/05 13:58:09 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n",
            "âœ” Loaded pretrained model on CPU!\n",
            "Image loaded: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:505: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "W1205 13:58:22.010000 11120 torch/fx/_symbolic_trace.py:52] is_fx_tracing will return true for both fx.symbolic_trace and torch.export. Please use is_fx_tracing_symbolic_tracing() for specifically fx.symbolic_trace or torch.compiler.is_compiling() for specifically torch.export/compile.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference complete!\n",
            "{'instances': Instances(num_instances=0, image_height=480, image_width=640, fields=[pred_boxes: Boxes(tensor([], size=(0, 4))), scores: tensor([]), pred_classes: tensor([], dtype=torch.int64)])}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "10) You are assigned to build a wildlife monitoring system to detect and track different animal species in a forest using Detectron2.\n",
        "Describe the end-to-end pipeline from data collection to deploying the model, and how you would handle challenges like occlusion or nighttime detection.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "d8AY1KWfkuCS",
        "outputId": "b128cbad-b00f-4056-f1ee-4333553f83a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n10) You are assigned to build a wildlife monitoring system to detect and track different animal species in a forest using Detectron2. \\nDescribe the end-to-end pipeline from data collection to deploying the model, and how you would handle challenges like occlusion or nighttime detection. \\n\\n->\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) You are assigned to build a wildlife monitoring system to detect and track different animal species in a forest using Detectron2.\n",
        "Describe the end-to-end pipeline from data collection to deploying the model, and how you would handle challenges like occlusion or nighttime detection.\n",
        "\n",
        "->\n",
        "\n",
        "Wildlife Monitoring System Using Detectron2 â€“ End-to-End Pipeline\n",
        "\n",
        "Building a wildlife monitoring system involves several stages, from collecting data to deploying a trained model in production. Detectron2 provides state-of-the-art tools for object detection, instance segmentation, and tracking, making it ideal for this application.\n",
        "\n",
        "\n",
        "\n",
        "1. Data Collection\n",
        "\n",
        "Sources of wildlife data:\n",
        "- Camera trap images and videos  \n",
        "- Drone footage  \n",
        "- Surveillance towers with high-resolution cameras  \n",
        "- Infrared / thermal cameras for nighttime monitoring  \n",
        "\n",
        "To build a robust model, data should include:\n",
        "- Day and night images  \n",
        "- Multiple species  \n",
        "- Occluded animals (behind vegetation)  \n",
        "- Weather variations (fog, rain, sunlight)\n",
        "\n",
        "\n",
        "\n",
        "2. Data Annotation\n",
        "\n",
        "Annotation tools:\n",
        "- LabelMe  \n",
        "- CVAT  \n",
        "- Roboflow  \n",
        "- LabelImg  \n",
        "\n",
        "Each image is annotated with:\n",
        "- Bounding boxes or segmentation masks  \n",
        "- Species labels  \n",
        "- Optional metadata (location, time, camera ID)\n",
        "\n",
        "Annotations must be exported in COCO format, which Detectron2 supports natively.\n",
        "\n",
        "\n",
        "\n",
        "3. Dataset Preprocessing\n",
        "\n",
        "Key preprocessing steps:\n",
        "- Remove unusable / blurry images  \n",
        "- Resize to consistent dimensions  \n",
        "- Split into train (70%), val (20%), test (10%)  \n",
        "\n",
        "Recommended augmentations:\n",
        "- RandomBrightness  \n",
        "- RandomFlip  \n",
        "- RandomRotation  \n",
        "- RandomContrast  \n",
        "- Custom augmentations (fog, low-light simulation)\n",
        "\n",
        "These help handle occlusion and nighttime conditions.\n",
        "\n",
        "\n",
        "\n",
        "4. Model Selection Using Detectron2\n",
        "\n",
        "Suitable Detectron2 models:\n",
        "- Faster R-CNN (R50-FPN) â€“ balanced accuracy and speed  \n",
        "- Mask R-CNN â€“ best for handling occlusion with segmentation  \n",
        "- RetinaNet â€“ faster real-time detection  \n",
        "\n",
        "Load pretrained weights:\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\n",
        "\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "\n",
        "Modify:\n",
        "- `NUM_CLASSES`\n",
        "- batch size  \n",
        "- learning rate  \n",
        "\n",
        "\n",
        "\n",
        "5. Training the Model\n",
        "\n",
        "Train with Detectron2â€™s `DefaultTrainer`:\n",
        "- Monitor training loss  \n",
        "- Use validation dataset to check generalization  \n",
        "- Apply augmentation to improve robustness  \n",
        "\n",
        "Handling Occlusion During Training\n",
        "- Include real examples of partially hidden animals  \n",
        "- Use segmentation (Mask R-CNN)  \n",
        "- Train with strong augmentations  \n",
        "\n",
        "\n",
        "\n",
        "6. Model Evaluation\n",
        "\n",
        "Metrics:\n",
        "- mAP at different IoU thresholds  \n",
        "- Recall for rare species  \n",
        "- Confusion matrix for misclassification  \n",
        "- Precisionâ€“Recall curves\n",
        "\n",
        "Visual inspection helps refine training.\n",
        "\n",
        "\n",
        "\n",
        "7. Deployment Pipeline\n",
        "\n",
        "Deployment options:\n",
        "- Edge devices (Jetson Nano, Raspberry Pi)  \n",
        "- Cloud servers (AWS, GCP)  \n",
        "- Drone-mounted inference systems  \n",
        "\n",
        "# Real-Time Tracking\n",
        "Integrate:\n",
        "- DeepSORT / SORT  \n",
        "- Kalman filter  \n",
        "- Optical flow  \n",
        "\n",
        "Pipeline after deployment:\n",
        "1. Capture camera feed  \n",
        "2. Detect animals with Detectron2  \n",
        "3. Track individuals  \n",
        "4. Log species + timestamp  \n",
        "5. Trigger alerts (e.g., endangered animal detected)\n",
        "\n",
        "\n",
        "\n",
        " 8. Handling Key Challenges\n",
        "\n",
        "ðŸŸ§ Occlusion\n",
        "- Use Mask R-CNN  \n",
        "- Train with occluded examples  \n",
        "- Use FPN to detect objects at multiple scales  \n",
        "- Tracking helps maintain identity across frames  \n",
        "\n",
        "ðŸŒ™ Nighttime Detection\n",
        "- Include thermal / infrared data  \n",
        "- Brightness and contrast augmentation  \n",
        "- Denoising techniques like CLAHE  \n",
        "- Use specialized low-light models if needed  \n",
        "\n",
        "ðŸ¦Œ Species Confusion\n",
        "- Better class-specific training data  \n",
        "- Longer fine-tuning  \n",
        "- Add more discriminative augmentations  \n",
        "\n",
        "\n",
        "\n",
        " 9. Continuous Improvement\n",
        "\n",
        "- Monitor false positives / false negatives  \n",
        "- Add new species to the dataset  \n",
        "- Retrain periodically with fresh camera trap data  \n",
        "- Implement model drift monitoring  \n",
        "\n",
        "\n",
        "\n",
        " Summary\n",
        "\n",
        "An end-to-end wildlife monitoring system using Detectron2 includes:\n",
        "\n",
        "1. Data Collection â€“ diverse forest imagery  \n",
        "2. Annotation â€“ COCO format  \n",
        "3. Preprocessing â€“ augmentation & splitting  \n",
        "4. Model Selection â€“ Faster/Mask R-CNN  \n",
        "5. Training â€“ robust detection  \n",
        "6. Evaluation â€“ mAP, IoU, PR curves  \n",
        "7. Deployment â€“ edge or cloud  \n",
        "8. Challenge Handling â€“ occlusion, night detection  \n",
        "9. Continuous Improvement â€“ periodic retraining  \n",
        "\n",
        "This system provides accurate, real-time wildlife monitoring using Detectron2."
      ],
      "metadata": {
        "id": "uxUiJs11ykTM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}