{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrP0jYxEfFBj"
      },
      "source": [
        " ## NLP Introduction & Text Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) What is Computational Linguistics and how does it relate to NLP?\n",
        "\n",
        "->\n",
        "\n",
        "Computational Linguistics (CL) is an interdisciplinary field that focuses on the scientific study of language using computational methods. It aims to model how human language works by combining insights from:\n",
        "\n",
        "- Linguistics (syntax, semantics, pragmatics)\n",
        "- Computer Science\n",
        "- Artificial Intelligence\n",
        "- Mathematics and logic\n",
        "\n",
        "The primary objective of Computational Linguistics is to formally represent linguistic knowledge so that it can be processed by machines. This includes building models for grammar, sentence structure, meaning representation, and discourse.\n",
        "\n",
        "Natural Language Processing (NLP), on the other hand, is an applied discipline that uses computational linguistic theories along with machine learning and deep learning techniques to build real-world language-processing systems.\n",
        "\n",
        " Relationship Between CL and NLP\n",
        "- Computational Linguistics provides the theoretical and linguistic foundation\n",
        "- NLP focuses on practical implementations and applications\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "> Computational Linguistics explains *how language works*  \n",
        "> NLP applies that knowledge to solve real-world problems\n",
        "\n",
        "Modern NLP systems often combine:\n",
        "- Linguistic rules from CL  \n",
        "- Statistical methods  \n",
        "- Neural networks  \n",
        "\n",
        "Thus, NLP can be seen as the engineering arm of Computational Linguistics.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " 2) Briefly describe the historical evolution of Natural Language Processing.\n",
        "\n",
        "->\n",
        "\n",
        "The development of NLP has progressed through several important stages:\n",
        "\n",
        " 1. Early Rule-Based Systems (1950sâ€“1970s)\n",
        "- Language processing was based on handcrafted grammatical rules.\n",
        "- Linguists manually wrote syntax and grammar rules.\n",
        "- Example: Early machine translation systems.\n",
        "- Limitations:  \n",
        "  - Difficult to scale  \n",
        "  - Could not handle ambiguity  \n",
        "  - High maintenance cost  \n",
        "\n",
        "\n",
        "\n",
        " 2. Statistical NLP Era (1980sâ€“2000s)\n",
        "- Shift from rules to probability-based models.\n",
        "- Use of large text corpora.\n",
        "- Common models:\n",
        "  - N-grams\n",
        "  - Hidden Markov Models (HMMs)\n",
        "  - Probabilistic Context-Free Grammars\n",
        "- Advantages: Better handling of uncertainty.\n",
        "- Limitations: Required large annotated datasets.\n",
        "\n",
        "\n",
        "\n",
        " 3. Machine Learning Era (2000sâ€“2010s)\n",
        "- Introduction of supervised learning algorithms.\n",
        "- Popular techniques:\n",
        "  - Support Vector Machines (SVMs)\n",
        "  - Conditional Random Fields (CRFs)\n",
        "  - Decision Trees\n",
        "- Feature engineering became critical.\n",
        "- Improved accuracy over purely statistical methods.\n",
        "\n",
        "\n",
        "\n",
        " 4. Deep Learning and Transformer Era (2015â€“Present)\n",
        "- Use of neural networks and word embeddings.\n",
        "- Breakthrough models:\n",
        "  - Word2Vec, GloVe\n",
        "  - RNNs, LSTMs\n",
        "  - Transformers (BERT, GPT)\n",
        "- Advantages:\n",
        "  - Context-aware understanding\n",
        "  - Minimal feature engineering\n",
        "  - State-of-the-art performance across NLP tasks\n",
        "\n",
        "\n",
        "\n",
        " 3) List and explain three major use cases of NLP in todayâ€™s tech industry.\n",
        "\n",
        "->\n",
        "\n",
        " 1. Conversational AI (Chatbots & Virtual Assistants)\n",
        "- Examples: Customer support bots, Siri, Alexa, Google Assistant.\n",
        "- NLP enables:\n",
        "  - Understanding user intent\n",
        "  - Context-aware conversation\n",
        "  - Natural language generation\n",
        "- Widely used in customer service and automation.\n",
        "\n",
        "\n",
        "\n",
        " 2. Machine Translation\n",
        "- Automatic translation between languages.\n",
        "- Examples: Google Translate, DeepL.\n",
        "- NLP techniques handle:\n",
        "  - Syntax differences\n",
        "  - Semantic meaning\n",
        "  - Cultural context\n",
        "- Transformers have significantly improved translation quality.\n",
        "\n",
        "\n",
        "\n",
        " 3. Sentiment Analysis and Opinion Mining\n",
        "- Identifies emotions and opinions in text.\n",
        "- Used in:\n",
        "  - Product reviews\n",
        "  - Social media monitoring\n",
        "  - Brand reputation analysis\n",
        "- Helps businesses make data-driven decisions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " 4) What is text normalization and why is it essential in text processing tasks?\n",
        "\n",
        "->\n",
        "\n",
        "Text normalization is the process of converting raw text into a standardized and consistent format so that it can be effectively analyzed by NLP models.\n",
        "\n",
        " Common Text Normalization Techniques\n",
        "- Converting text to lowercase  \n",
        "- Removing punctuation and special characters  \n",
        "- Expanding contractions (e.g., *don't â†’ do not*)  \n",
        "- Removing stopwords  \n",
        "- Handling numbers and symbols  \n",
        "\n",
        " Why Text Normalization Is Essential\n",
        "- Reduces noise and inconsistencies\n",
        "- Ensures uniform word representation\n",
        "- Improves model accuracy and efficiency\n",
        "- Simplifies feature extraction\n",
        "- Prevents duplicate representations of the same word\n",
        "\n",
        "Example:\n",
        "```\n",
        "\"Iâ€™m Loving NLP!!!\" â†’ \"i am loving nlp\"\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        " 5) Compare and contrast stemming and lemmatization with suitable examples.\n",
        "\n",
        "->\n",
        "\n",
        "Both stemming and lemmatization are techniques used to reduce words to their base form, but they differ significantly in approach and accuracy.\n",
        "\n",
        "\n",
        "\n",
        " Stemming\n",
        "- Uses simple rule-based heuristics.\n",
        "- Removes word suffixes without understanding context.\n",
        "- Output may not be a valid dictionary word.\n",
        "\n",
        "Examples:\n",
        "- *running â†’ run*\n",
        "- *studies â†’ studi*\n",
        "- *connected â†’ connect*\n",
        "\n",
        "Advantages: Fast and computationally cheap  \n",
        "Disadvantages: Less accurate, may produce incorrect roots\n",
        "\n",
        "\n",
        "\n",
        " Lemmatization\n",
        "- Uses linguistic rules and vocabulary.\n",
        "- Considers context and part-of-speech.\n",
        "- Always produces a valid dictionary word.\n",
        "\n",
        "Examples:\n",
        "- *running â†’ run*\n",
        "- *studies â†’ study*\n",
        "- *better â†’ good*\n",
        "\n",
        "Advantages: More accurate and meaningful  \n",
        "Disadvantages: Slower and more computationally expensive\n",
        "\n",
        "\n",
        "\n",
        " Comparison Table\n",
        "\n",
        "| Aspect | Stemming | Lemmatization |\n",
        "|------|----------|---------------|\n",
        "| Approach | Rule-based | Linguistic + dictionary-based |\n",
        "| Context awareness | No | Yes |\n",
        "| Output | May be invalid | Always valid |\n",
        "| Speed | Fast | Slower |\n",
        "| Accuracy | Lower | Higher |"
      ],
      "metadata": {
        "id": "j_CqvK0ml_Ta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MJrAqKpqmX0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6668d3-036b-4b8c-fc2b-1c08df9ad1f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Email Addresses:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "6) Write a Python program that uses regular expressions (regex) to extract all\n",
        "email addresses from the following block of text:\n",
        "\n",
        "â€œHello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.â€\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "import re\n",
        "\n",
        "# Given text\n",
        "text = \"\"\"\n",
        "Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at\n",
        "john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us.\n",
        "For partnership inquiries, email partners@xyz.biz.\n",
        "\"\"\"\n",
        "\n",
        "# Regular expression pattern for email addresses\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Extract emails using findall\n",
        "emails = re.findall(email_pattern, text)\n",
        "\n",
        "# Print extracted email addresses\n",
        "print(\"Extracted Email Addresses:\")\n",
        "for email in emails:\n",
        "    print(email)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vKYRM7camlV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839d20d8-a9a1-46c9-9fc9-4271d3a21a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Frequency Distribution:\n",
            "Natural : 1\n",
            "Language : 1\n",
            "Processing : 1\n",
            "( : 1\n",
            "NLP : 3\n",
            ") : 1\n",
            "is : 2\n",
            "a : 1\n",
            "fascinating : 1\n",
            "field : 1\n",
            "that : 1\n",
            "combines : 1\n",
            "linguistics : 1\n",
            ", : 7\n",
            "computer : 1\n",
            "science : 1\n",
            "and : 3\n",
            "artificial : 1\n",
            "intelligence : 1\n",
            ". : 4\n",
            "It : 1\n",
            "enables : 1\n",
            "machines : 1\n",
            "to : 1\n",
            "understand : 1\n",
            "interpret : 1\n",
            "generate : 1\n",
            "human : 1\n",
            "language : 1\n",
            "Applications : 1\n",
            "of : 2\n",
            "include : 1\n",
            "chatbots : 1\n",
            "sentiment : 1\n",
            "analysis : 1\n",
            "machine : 1\n",
            "translation : 1\n",
            "As : 1\n",
            "technology : 1\n",
            "advances : 1\n",
            "the : 1\n",
            "role : 1\n",
            "in : 1\n",
            "modern : 1\n",
            "solutions : 1\n",
            "becoming : 1\n",
            "increasingly : 1\n",
            "critical : 1\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "7)  Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "\n",
        "â€œNatural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP in\n",
        "modern solutions is becoming increasingly critical.â€\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "# Import required libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download required NLTK resources (run once)\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Given paragraph\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP in\n",
        "modern solutions is becoming increasingly critical.\n",
        "\"\"\"\n",
        "\n",
        "# Step 1: Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# Step 2: Frequency Distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, freq in freq_dist.items():\n",
        "    print(f\"{word} : {freq}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "6MbiezoHmsjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76a21a6e-6210-4f05-99b3-3e304808d4ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Proper Nouns Identified:\n",
            "Apple -> PROPER_NOUN\n",
            "Steve -> PROPER_NOUN\n",
            "Jobs -> PROPER_NOUN\n",
            "California -> PROPER_NOUN\n",
            "Tim -> PROPER_NOUN\n",
            "Cook -> PROPER_NOUN\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "8)  Create a custom annotator using spaCy or NLTK that identifies and labels proper nouns in a given text.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "# Install spaCy\n",
        "!pip install spacy --quiet\n",
        "\n",
        "# Download English model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Custom annotator function\n",
        "def proper_noun_annotator(text):\n",
        "    doc = nlp(text)\n",
        "    annotations = []\n",
        "    for token in doc:\n",
        "        if token.pos_ == \"PROPN\":\n",
        "            annotations.append((token.text, \"PROPER_NOUN\"))\n",
        "    return annotations\n",
        "\n",
        "# Sample text\n",
        "text = \"Apple was founded by Steve Jobs in California and is now led by Tim Cook.\"\n",
        "\n",
        "# Run annotator\n",
        "result = proper_noun_annotator(text)\n",
        "\n",
        "print(\"Proper Nouns Identified:\")\n",
        "for word, label in result:\n",
        "    print(f\"{word} -> {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "9) Using Genism, demonstrate how to train a simple Word2Vec model on the\n",
        "following dataset consisting of example sentences:\n",
        "\n",
        "dataset = [\n",
        "  \"Natural language processing enables computers to understand human language\",\n",
        "\n",
        "  \"Word embeddings are a type of word representation that allows words with similar\n",
        "  meaning to have similar representation\",\n",
        "\n",
        "  \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "\n",
        "  \"Text preprocessing is a critical step before training word embeddings\",\n",
        "\n",
        "  \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using\n",
        "Gensim.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "# Install Gensim\n",
        "!pip install gensim --quiet\n",
        "\n",
        "# Import required libraries\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Given dataset\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# Step 1: Text Preprocessing and Tokenization\n",
        "# - lowercasing\n",
        "# - removing punctuation\n",
        "# - tokenizing into words\n",
        "tokenized_data = [simple_preprocess(sentence) for sentence in dataset]\n",
        "\n",
        "print(\"Tokenized Dataset:\")\n",
        "for sentence in tokenized_data:\n",
        "    print(sentence)\n",
        "\n",
        "# Step 2: Train Word2Vec model\n",
        "model = Word2Vec(\n",
        "    sentences=tokenized_data,\n",
        "    vector_size=50,   # size of word vectors\n",
        "    window=5,         # context window size\n",
        "    min_count=1,      # include all words\n",
        "    workers=2,        # number of CPU threads\n",
        "    sg=0              # 0 = CBOW, 1 = Skip-gram\n",
        ")\n",
        "\n",
        "# Step 3: Access word vectors\n",
        "print(\"\\nWord Vector for 'language':\")\n",
        "print(model.wv['language'])\n",
        "\n",
        "# Step 4: Find similar words\n",
        "print(\"\\nWords similar to 'word':\")\n",
        "print(model.wv.most_similar('word'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVHIfAZRklEA",
        "outputId": "f924b676-c311-45ab-c200-b3d70967a64c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTokenized Dataset:\n",
            "['natural', 'language', 'processing', 'enables', 'computers', 'to', 'understand', 'human', 'language']\n",
            "['word', 'embeddings', 'are', 'type', 'of', 'word', 'representation', 'that', 'allows', 'words', 'with', 'similar', 'meaning', 'to', 'have', 'similar', 'representation']\n",
            "['word', 'vec', 'is', 'popular', 'word', 'embedding', 'technique', 'used', 'in', 'many', 'nlp', 'applications']\n",
            "['text', 'preprocessing', 'is', 'critical', 'step', 'before', 'training', 'word', 'embeddings']\n",
            "['tokenization', 'and', 'normalization', 'help', 'clean', 'raw', 'text', 'for', 'modeling']\n",
            "\n",
            "Word Vector for 'language':\n",
            "[ 0.00855287  0.00015212 -0.01916856 -0.01933109 -0.01229639 -0.00025714\n",
            "  0.00399483  0.01886394  0.0111687  -0.00858139  0.00055663  0.00992872\n",
            "  0.01539662 -0.00228845  0.00864684 -0.01162876 -0.00160838  0.0162001\n",
            " -0.00472013 -0.01932691  0.01155852 -0.00785964 -0.00244575  0.01996103\n",
            " -0.0045127  -0.00951413 -0.01065877  0.01396178 -0.01141774  0.00422733\n",
            " -0.01051132  0.01224143  0.00871461  0.00521271 -0.00298217 -0.00549213\n",
            "  0.01798587  0.01043155 -0.00432504 -0.01894062 -0.0148521  -0.00212748\n",
            " -0.00158989 -0.00512582  0.01936544 -0.00091704  0.01174752 -0.01489517\n",
            " -0.00501215 -0.01109973]\n",
            "\n",
            "Words similar to 'word':\n",
            "[('step', 0.2706873416900635), ('processing', 0.2548932731151581), ('with', 0.24128952622413635), ('and', 0.211445614695549), ('many', 0.18664862215518951), ('human', 0.1767798215150833), ('clean', 0.16752134263515472), ('nlp', 0.16078214347362518), ('normalization', 0.15081925690174103), ('vec', 0.14548535645008087)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) Imagine you are a data scientist at a fintech startup. Youâ€™ve been tasked\n",
        "with analyzing customer feedback. Outline the steps you would take to clean, process,\n",
        "and extract useful insights using NLP techniques from thousands of customer reviews.\n",
        "\n",
        "->\n",
        "\n",
        "Analyzing Customer Feedback Using NLP in a Fintech Startup\n",
        "\n",
        "As a data scientist in a fintech startup, analyzing thousands of customer reviews requires a systematic NLP pipeline to clean, process, and extract actionable insights. The goal is to understand customer sentiment, identify recurring issues, and support data-driven decision-making.\n",
        "\n",
        "\n",
        "\n",
        "1. Data Collection and Understanding\n",
        "- Collect customer feedback from multiple sources:\n",
        "  - App store reviews\n",
        "  - In-app feedback forms\n",
        "  - Emails and support tickets\n",
        "  - Social media mentions\n",
        "- Inspect the data for:\n",
        "  - Noise (HTML tags, emojis, URLs)\n",
        "  - Duplicates\n",
        "  - Missing or empty reviews\n",
        "- Store the data in a structured format (CSV / database).\n",
        "\n",
        "\n",
        "2. Data Cleaning\n",
        "Cleaning is essential to remove noise and inconsistencies.\n",
        "\n",
        "Typical Cleaning Steps\n",
        "- Convert text to lowercase\n",
        "- Remove:\n",
        "  - HTML tags\n",
        "  - URLs\n",
        "  - Email addresses\n",
        "  - Special characters and numbers\n",
        "- Remove duplicate reviews\n",
        "- Handle spelling variations and contractions\n",
        "\n",
        "Example:\n",
        "\"Great App!!! ðŸ’³ðŸ’°\" â†’ \"great app\"\n",
        "\n",
        "\n",
        "3. Text Preprocessing\n",
        "Prepare the cleaned text for analysis.\n",
        "\n",
        "Key Preprocessing Techniques\n",
        "- **Tokenization** â€“ split text into words or sentences\n",
        "- **Stopword Removal** â€“ remove common words like *is, the, and*\n",
        "- **Stemming / Lemmatization** â€“ reduce words to base form\n",
        "- **Text Normalization** â€“ standardize text representation\n",
        "\n",
        "Example:\n",
        "\"transactions were failing repeatedly\"\n",
        "â†’ [\"transaction\", \"fail\", \"repeatedly\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4. Feature Extraction\n",
        "Convert text into numerical form for analysis.\n",
        "\n",
        "Common Techniques\n",
        "- Bag of Words (BoW)\n",
        "- TF-IDF (Term Frequencyâ€“Inverse Document Frequency)\n",
        "- Word Embeddings (Word2Vec, GloVe)\n",
        "- Contextual embeddings (BERT)\n",
        "\n",
        "This step allows machine learning models to process text data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5. Exploratory Text Analysis\n",
        "Gain high-level insights before modeling.\n",
        "\n",
        "Techniques\n",
        "- Word frequency analysis\n",
        "- N-gram analysis (bigrams, trigrams)\n",
        "- Keyword extraction\n",
        "- Word clouds\n",
        "\n",
        "This helps identify:\n",
        "- Frequently mentioned features (e.g., *payments, login, refunds*)\n",
        "- Common complaints or praises\n",
        "\n",
        "\n",
        "\n",
        "6. Sentiment Analysis\n",
        "Determine customer emotions and opinions.\n",
        "\n",
        "Approaches\n",
        "- Rule-based (lexicon-based sentiment analysis)\n",
        "- Machine learning classifiers\n",
        "- Deep learning models (LSTM, BERT)\n",
        "\n",
        "\n",
        "Output\n",
        "- Positive feedback â†’ highlights strengths\n",
        "- Negative feedback â†’ identifies pain points\n",
        "- Neutral feedback â†’ informational reviews\n",
        "\n",
        "Sentiment trends can be tracked over time.\n",
        "\n",
        "\n",
        "7. Topic Modeling\n",
        "Discover hidden themes in large volumes of reviews.\n",
        "\n",
        "Techniques\n",
        "- Latent Dirichlet Allocation (LDA)\n",
        "- Non-negative Matrix Factorization (NMF)\n",
        "\n",
        "Example Topics\n",
        "- Payment failures\n",
        "- App performance issues\n",
        "- Customer support experience\n",
        "- Security concerns\n",
        "\n",
        "This helps prioritize product improvements.\n",
        "\n",
        "\n",
        "\n",
        "8. Aspect-Based Sentiment Analysis\n",
        "Analyze sentiment for specific product aspects.\n",
        "\n",
        "Example:\n",
        "- *UI* â†’ Positive\n",
        "- *Transaction speed* â†’ Negative\n",
        "- *Customer support* â†’ Mixed\n",
        "\n",
        "This is especially useful for fintech products with multiple features.\n",
        "\n",
        "\n",
        "\n",
        "9. Insight Extraction and Visualization\n",
        "Present insights in a business-friendly manner.\n",
        "\n",
        "Dashboards and Reports\n",
        "- Sentiment distribution charts\n",
        "- Top complaint categories\n",
        "- Trend analysis over time\n",
        "- Alerts for sudden spikes in negative feedback\n",
        "\n",
        "Tools: Tableau, Power BI, Python dashboards.\n",
        "\n",
        "\n",
        "10. Actionable Decision-Making\n",
        "Use NLP insights to:\n",
        "- Improve app features\n",
        "- Fix recurring bugs\n",
        "- Enhance customer support workflows\n",
        "- Inform product roadmaps\n",
        "- Measure impact of new releases"
      ],
      "metadata": {
        "id": "uxUiJs11ykTM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}