{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrP0jYxEfFBj"
      },
      "source": [
        "## Useful NLP Libraries & Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_CqvK0ml_Ta"
      },
      "source": [
        "1) What is Computational Linguistics and how does it relate to NLP?\n",
        "\n",
        "->\n",
        "\n",
        "Computational Linguistics (CL) is an interdisciplinary field that focuses on the scientific study of language using computational methods. It aims to model how human language works by combining insights from:\n",
        "\n",
        "- Linguistics (syntax, semantics, pragmatics)\n",
        "- Computer Science\n",
        "- Artificial Intelligence\n",
        "- Mathematics and logic\n",
        "\n",
        "The primary objective of Computational Linguistics is to formally represent linguistic knowledge so that it can be processed by machines. This includes building models for grammar, sentence structure, meaning representation, and discourse.\n",
        "\n",
        "Natural Language Processing (NLP), on the other hand, is an applied discipline that uses computational linguistic theories along with machine learning and deep learning techniques to build real-world language-processing systems.\n",
        "\n",
        " Relationship Between CL and NLP\n",
        "- Computational Linguistics provides the theoretical and linguistic foundation\n",
        "- NLP focuses on practical implementations and applications\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "> Computational Linguistics explains *how language works*  \n",
        "> NLP applies that knowledge to solve real-world problems\n",
        "\n",
        "Modern NLP systems often combine:\n",
        "- Linguistic rules from CL  \n",
        "- Statistical methods  \n",
        "- Neural networks  \n",
        "\n",
        "Thus, NLP can be seen as the engineering arm of Computational Linguistics.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " 2) Briefly describe the historical evolution of Natural Language Processing.\n",
        "\n",
        "->\n",
        "\n",
        "The development of NLP has progressed through several important stages:\n",
        "\n",
        " 1. Early Rule-Based Systems (1950s–1970s)\n",
        "- Language processing was based on handcrafted grammatical rules.\n",
        "- Linguists manually wrote syntax and grammar rules.\n",
        "- Example: Early machine translation systems.\n",
        "- Limitations:  \n",
        "  - Difficult to scale  \n",
        "  - Could not handle ambiguity  \n",
        "  - High maintenance cost  \n",
        "\n",
        "\n",
        "\n",
        " 2. Statistical NLP Era (1980s–2000s)\n",
        "- Shift from rules to probability-based models.\n",
        "- Use of large text corpora.\n",
        "- Common models:\n",
        "  - N-grams\n",
        "  - Hidden Markov Models (HMMs)\n",
        "  - Probabilistic Context-Free Grammars\n",
        "- Advantages: Better handling of uncertainty.\n",
        "- Limitations: Required large annotated datasets.\n",
        "\n",
        "\n",
        "\n",
        " 3. Machine Learning Era (2000s–2010s)\n",
        "- Introduction of supervised learning algorithms.\n",
        "- Popular techniques:\n",
        "  - Support Vector Machines (SVMs)\n",
        "  - Conditional Random Fields (CRFs)\n",
        "  - Decision Trees\n",
        "- Feature engineering became critical.\n",
        "- Improved accuracy over purely statistical methods.\n",
        "\n",
        "\n",
        "\n",
        " 4. Deep Learning and Transformer Era (2015–Present)\n",
        "- Use of neural networks and word embeddings.\n",
        "- Breakthrough models:\n",
        "  - Word2Vec, GloVe\n",
        "  - RNNs, LSTMs\n",
        "  - Transformers (BERT, GPT)\n",
        "- Advantages:\n",
        "  - Context-aware understanding\n",
        "  - Minimal feature engineering\n",
        "  - State-of-the-art performance across NLP tasks\n",
        "\n",
        "\n",
        "\n",
        " 3) List and explain three major use cases of NLP in today’s tech industry.\n",
        "\n",
        "->\n",
        "\n",
        " 1. Conversational AI (Chatbots & Virtual Assistants)\n",
        "- Examples: Customer support bots, Siri, Alexa, Google Assistant.\n",
        "- NLP enables:\n",
        "  - Understanding user intent\n",
        "  - Context-aware conversation\n",
        "  - Natural language generation\n",
        "- Widely used in customer service and automation.\n",
        "\n",
        "\n",
        "\n",
        " 2. Machine Translation\n",
        "- Automatic translation between languages.\n",
        "- Examples: Google Translate, DeepL.\n",
        "- NLP techniques handle:\n",
        "  - Syntax differences\n",
        "  - Semantic meaning\n",
        "  - Cultural context\n",
        "- Transformers have significantly improved translation quality.\n",
        "\n",
        "\n",
        "\n",
        " 3. Sentiment Analysis and Opinion Mining\n",
        "- Identifies emotions and opinions in text.\n",
        "- Used in:\n",
        "  - Product reviews\n",
        "  - Social media monitoring\n",
        "  - Brand reputation analysis\n",
        "- Helps businesses make data-driven decisions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " 4) What is text normalization and why is it essential in text processing tasks?\n",
        "\n",
        "->\n",
        "\n",
        "Text normalization is the process of converting raw text into a standardized and consistent format so that it can be effectively analyzed by NLP models.\n",
        "\n",
        " Common Text Normalization Techniques\n",
        "- Converting text to lowercase  \n",
        "- Removing punctuation and special characters  \n",
        "- Expanding contractions (e.g., *don't → do not*)  \n",
        "- Removing stopwords  \n",
        "- Handling numbers and symbols  \n",
        "\n",
        " Why Text Normalization Is Essential\n",
        "- Reduces noise and inconsistencies\n",
        "- Ensures uniform word representation\n",
        "- Improves model accuracy and efficiency\n",
        "- Simplifies feature extraction\n",
        "- Prevents duplicate representations of the same word\n",
        "\n",
        "Example:\n",
        "```\n",
        "\"I’m Loving NLP!!!\" → \"i am loving nlp\"\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        " 5) Compare and contrast stemming and lemmatization with suitable examples.\n",
        "\n",
        "->\n",
        "\n",
        "Both stemming and lemmatization are techniques used to reduce words to their base form, but they differ significantly in approach and accuracy.\n",
        "\n",
        "\n",
        "\n",
        " Stemming\n",
        "- Uses simple rule-based heuristics.\n",
        "- Removes word suffixes without understanding context.\n",
        "- Output may not be a valid dictionary word.\n",
        "\n",
        "Examples:\n",
        "- *running → run*\n",
        "- *studies → studi*\n",
        "- *connected → connect*\n",
        "\n",
        "Advantages: Fast and computationally cheap  \n",
        "Disadvantages: Less accurate, may produce incorrect roots\n",
        "\n",
        "\n",
        "\n",
        " Lemmatization\n",
        "- Uses linguistic rules and vocabulary.\n",
        "- Considers context and part-of-speech.\n",
        "- Always produces a valid dictionary word.\n",
        "\n",
        "Examples:\n",
        "- *running → run*\n",
        "- *studies → study*\n",
        "- *better → good*\n",
        "\n",
        "Advantages: More accurate and meaningful  \n",
        "Disadvantages: Slower and more computationally expensive\n",
        "\n",
        "\n",
        "\n",
        " Comparison Table\n",
        "\n",
        "| Aspect | Stemming | Lemmatization |\n",
        "|------|----------|---------------|\n",
        "| Approach | Rule-based | Linguistic + dictionary-based |\n",
        "| Context awareness | No | Yes |\n",
        "| Output | May be invalid | Always valid |\n",
        "| Speed | Fast | Slower |\n",
        "| Accuracy | Lower | Higher |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MJrAqKpqmX0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982d9b43-6505-4aca-b767-267eb4f2f36a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Results:\n",
            "Polarity: 0.21742424242424244\n",
            "Subjectivity: 0.6511363636363636\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "6) Write a Python program using TextBlob to perform sentiment analysis on the following paragraph of text:\n",
        "\n",
        "“I had a great experience using the new mobile banking app. The interface is intuitive, and customer support was quick to resolve my issue.\n",
        "However, the app did crash once during a transaction, which was frustrating\"\n",
        "\n",
        "Your program should print out the polarity and subjectivity scores.\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Install TextBlob\n",
        "!pip install textblob --quiet\n",
        "\n",
        "# Import TextBlob\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Given paragraph\n",
        "text = \"\"\"\n",
        "I had a great experience using the new mobile banking app.\n",
        "The interface is intuitive, and customer support was quick to resolve my issue.\n",
        "However, the app did crash once during a transaction, which was frustrating.\n",
        "\"\"\"\n",
        "\n",
        "# Create TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Perform sentiment analysis\n",
        "sentiment = blob.sentiment\n",
        "\n",
        "# Print results\n",
        "print(\"Sentiment Analysis Results:\")\n",
        "print(\"Polarity:\", sentiment.polarity)\n",
        "print(\"Subjectivity:\", sentiment.subjectivity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vKYRM7camlV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed7fabe-0261-412c-ad2e-b2f0c924d892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Frequency Distribution:\n",
            "Natural : 1\n",
            "Language : 1\n",
            "Processing : 1\n",
            "( : 1\n",
            "NLP : 3\n",
            ") : 1\n",
            "is : 2\n",
            "a : 1\n",
            "fascinating : 1\n",
            "field : 1\n",
            "that : 1\n",
            "combines : 1\n",
            "linguistics : 1\n",
            ", : 7\n",
            "computer : 1\n",
            "science : 1\n",
            "and : 3\n",
            "artificial : 1\n",
            "intelligence : 1\n",
            ". : 4\n",
            "It : 1\n",
            "enables : 1\n",
            "machines : 1\n",
            "to : 1\n",
            "understand : 1\n",
            "interpret : 1\n",
            "generate : 1\n",
            "human : 1\n",
            "language : 1\n",
            "Applications : 1\n",
            "of : 2\n",
            "include : 1\n",
            "chatbots : 1\n",
            "sentiment : 1\n",
            "analysis : 1\n",
            "machine : 1\n",
            "translation : 1\n",
            "As : 1\n",
            "technology : 1\n",
            "advances : 1\n",
            "the : 1\n",
            "role : 1\n",
            "in : 1\n",
            "modern : 1\n",
            "solutions : 1\n",
            "becoming : 1\n",
            "increasingly : 1\n",
            "critical : 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "7)  Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK:\n",
        "\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence.\n",
        "It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and\n",
        "machine translation.As technology advances, the role of NLP in modern solutions is becoming increasingly critical.”\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Import required libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download required NLTK resources (run once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Given paragraph\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence.\n",
        "It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and\n",
        "machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\n",
        "\"\"\"\n",
        "\n",
        "# Step 1: Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# Step 2: Frequency Distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, freq in freq_dist.items():\n",
        "    print(f\"{word} : {freq}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6MbiezoHmsjw",
        "outputId": "31befe7f-3242-465b-d5c9-2be480dfa8bd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Index:\n",
            "{'this': 1, 'i': 2, 'is': 3, 'love': 4, 'project': 5, 'an': 6, 'amazing': 7, 'experience': 8, 'hate': 9, 'waiting': 10, 'in': 11, 'line': 12, 'the': 13, 'worst': 14, 'service': 15, 'absolutely': 16, 'fantastic': 17}\n",
            "\n",
            "Padded Sequences:\n",
            "[[ 2  4  1  5  0  0]\n",
            " [ 1  3  6  7  8  0]\n",
            " [ 2  9 10 11 12  0]\n",
            " [ 1  3 13 14 15  0]\n",
            " [16 17  0  0  0  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.4000 - loss: 0.6945\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6000 - loss: 0.6932\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6000 - loss: 0.6920\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6000 - loss: 0.6909\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6000 - loss: 0.6898\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6000 - loss: 0.6887\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6000 - loss: 0.6876\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.6000 - loss: 0.6865\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6000 - loss: 0.6854\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6000 - loss: 0.6843\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6000 - loss: 0.6831\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6000 - loss: 0.6819\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6000 - loss: 0.6806\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6792\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6000 - loss: 0.6777\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6000 - loss: 0.6761\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6000 - loss: 0.6744\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6000 - loss: 0.6726\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6000 - loss: 0.6707\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6000 - loss: 0.6686\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step\n",
            "\n",
            "Test Sentence: I really love this experience\n",
            "Predicted Sentiment Score: 0.54307157\n",
            "Predicted Label: 1\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "8)  Implement a basic LSTM model in Keras for a text classification task using\n",
        "the following dummy dataset. Your model should classify sentences as either positive\n",
        "(1) or negative (0).\n",
        "\n",
        "# Dataset\n",
        "texts = [\n",
        "“I love this project”, #Positive\n",
        "“This is an amazing experience”, #Positive\n",
        "“I hate waiting in line”, #Negative\n",
        "“This is the worst service”, #Negative\n",
        "“Absolutely fantastic!” #Positive\n",
        "]\n",
        "\n",
        "labels = [1, 1, 0, 0, 1]\n",
        "\n",
        "Preprocess the text, tokenize it, pad sequences, and build an LSTM model to train on\n",
        "this data. You may use Keras with TensorFlow backend.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Install TensorFlow (if not already installed)\n",
        "!pip install tensorflow --quiet\n",
        "\n",
        "# -----------------------------\n",
        "# Import required libraries\n",
        "# -----------------------------\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset\n",
        "# -----------------------------\n",
        "texts = [\n",
        "    \"I love this project\",                # Positive\n",
        "    \"This is an amazing experience\",      # Positive\n",
        "    \"I hate waiting in line\",              # Negative\n",
        "    \"This is the worst service\",           # Negative\n",
        "    \"Absolutely fantastic\"                # Positive\n",
        "]\n",
        "\n",
        "labels = [1, 1, 0, 0, 1]\n",
        "\n",
        "# Convert labels to NumPy array (IMPORTANT)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Tokenization\n",
        "# -----------------------------\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(\"Word Index:\")\n",
        "print(word_index)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: Padding Sequences\n",
        "# -----------------------------\n",
        "max_length = 6\n",
        "padded_sequences = pad_sequences(\n",
        "    sequences,\n",
        "    maxlen=max_length,\n",
        "    padding='post'\n",
        ")\n",
        "\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded_sequences)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: Build LSTM Model\n",
        "# -----------------------------\n",
        "vocab_size = len(word_index) + 1  # +1 for padding token\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=16, input_length=max_length),\n",
        "    LSTM(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Compile Model\n",
        "# -----------------------------\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# -----------------------------\n",
        "# Step 5: Train Model\n",
        "# -----------------------------\n",
        "model.fit(\n",
        "    padded_sequences,\n",
        "    labels,\n",
        "    epochs=20,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 6: Test the Model\n",
        "# -----------------------------\n",
        "test_text = [\"I really love this experience\"]\n",
        "test_seq = tokenizer.texts_to_sequences(test_text)\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "prediction = model.predict(test_pad)\n",
        "\n",
        "print(\"\\nTest Sentence:\", test_text[0])\n",
        "print(\"Predicted Sentiment Score:\", prediction[0][0])\n",
        "print(\"Predicted Label:\", 1 if prediction[0][0] > 0.5 else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OVHIfAZRklEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e256510-17c0-4105-8d42-483ddbd6876e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Tokens and Lemmas:\n",
            "Token: Homi                 Lemma: Homi\n",
            "Token: Jehangir             Lemma: Jehangir\n",
            "Token: Bhaba                Lemma: Bhaba\n",
            "Token: was                  Lemma: be\n",
            "Token: an                   Lemma: an\n",
            "Token: Indian               Lemma: indian\n",
            "Token: nuclear              Lemma: nuclear\n",
            "Token: physicist            Lemma: physicist\n",
            "Token: who                  Lemma: who\n",
            "Token: played               Lemma: play\n",
            "Token: a                    Lemma: a\n",
            "Token: key                  Lemma: key\n",
            "Token: role                 Lemma: role\n",
            "Token: in                   Lemma: in\n",
            "Token: the                  Lemma: the\n",
            "Token: development          Lemma: development\n",
            "Token: of                   Lemma: of\n",
            "Token: India                Lemma: India\n",
            "Token: ’s                   Lemma: ’s\n",
            "Token: atomic               Lemma: atomic\n",
            "Token: energy               Lemma: energy\n",
            "Token: program              Lemma: program\n",
            "Token: .                    Lemma: .\n",
            "Token: He                   Lemma: he\n",
            "Token: was                  Lemma: be\n",
            "Token: the                  Lemma: the\n",
            "Token: founding             Lemma: found\n",
            "Token: director             Lemma: director\n",
            "Token: of                   Lemma: of\n",
            "Token: the                  Lemma: the\n",
            "Token: Tata                 Lemma: Tata\n",
            "Token: Institute            Lemma: Institute\n",
            "Token: of                   Lemma: of\n",
            "Token: Fundamental          Lemma: Fundamental\n",
            "Token: Research             Lemma: Research\n",
            "Token: (                    Lemma: (\n",
            "Token: TIFR                 Lemma: TIFR\n",
            "Token: )                    Lemma: )\n",
            "Token: and                  Lemma: and\n",
            "Token: was                  Lemma: be\n",
            "Token: instrumental         Lemma: instrumental\n",
            "Token: in                   Lemma: in\n",
            "Token: establishing         Lemma: establish\n",
            "Token: the                  Lemma: the\n",
            "Token: Atomic               Lemma: Atomic\n",
            "Token: Energy               Lemma: Energy\n",
            "Token: Commission           Lemma: Commission\n",
            "Token: of                   Lemma: of\n",
            "Token: India                Lemma: India\n",
            "Token: .                    Lemma: .\n",
            "\n",
            "Named Entities:\n",
            "Entity: Indian                                        Label: NORP\n",
            "Entity: India                                         Label: GPE\n",
            "Entity: the Tata\n",
            "Institute of Fundamental Research    Label: ORG\n",
            "Entity: Atomic Energy Commission of India             Label: ORG\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "9) Using spaCy, build a simple NLP pipeline that includes tokenization,\n",
        "lemmatization, and entity recognition. Use the following paragraph as your dataset:\n",
        "\n",
        "“Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
        "development of India’s atomic energy program. He was the founding director of the Tata\n",
        "Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
        "Atomic Energy Commission of India.”\n",
        "\n",
        "Write a Python program that processes this text using spaCy, then prints tokens, their\n",
        "lemmas, and any named entities found.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Install spaCy\n",
        "!pip install spacy --quiet\n",
        "\n",
        "# Download the English language model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# -----------------------------\n",
        "# Import spaCy and load model\n",
        "# -----------------------------\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# -----------------------------\n",
        "# Given paragraph\n",
        "# -----------------------------\n",
        "text = \"\"\"\n",
        "Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
        "development of India’s atomic energy program. He was the founding director of the Tata\n",
        "Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
        "Atomic Energy Commission of India.\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------\n",
        "# Process text using spaCy\n",
        "# -----------------------------\n",
        "doc = nlp(text)\n",
        "\n",
        "# -----------------------------\n",
        "# Tokenization and Lemmatization\n",
        "# -----------------------------\n",
        "print(\"Tokens and Lemmas:\")\n",
        "for token in doc:\n",
        "    if not token.is_space:\n",
        "        print(f\"Token: {token.text:<20} Lemma: {token.lemma_}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Named Entity Recognition\n",
        "# -----------------------------\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text:<45} Label: {ent.label_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxUiJs11ykTM"
      },
      "source": [
        "10) You are working on a chatbot for a mental health platform. Explain how\n",
        "you would leverage LSTM or GRU networks along with libraries like spaCy or Stanford NLP to understand and respond to user input effectively. Detail your architecture, data preprocessing pipeline, and any ethical considerations.\n",
        "\n",
        "->\n",
        "\n",
        "Designing a Mental Health Chatbot Using LSTM / GRU and NLP Libraries\n",
        "\n",
        "When building a chatbot for a mental health platform, the goal is not only to understand user input accurately but also to respond in a safe, empathetic, and ethical manner. Recurrent neural networks such as LSTM or GRU, combined with NLP libraries like spaCy or Stanford NLP, provide an effective foundation for such systems.\n",
        "\n",
        " 1. Overall System Architecture\n",
        "\n",
        "A typical mental health chatbot architecture consists of the following components:\n",
        "\n",
        "1. User Input Layer\n",
        "2. Text Preprocessing & NLP Pipeline\n",
        "3. Sequence Modeling (LSTM / GRU)\n",
        "4. Intent & Emotion Classification\n",
        "5. Response Generation or Retrieval\n",
        "6. Safety & Ethics Layer\n",
        "7. User Response Output\n",
        "\n",
        "\n",
        "\n",
        " 2. Data Preprocessing Pipeline\n",
        "\n",
        "Preprocessing is critical because user input may be informal, emotional, or unstructured.\n",
        "\n",
        "Steps in Preprocessing\n",
        "- Text Cleaning\n",
        "  - Lowercasing\n",
        "  - Removing unnecessary punctuation\n",
        "  - Handling emojis and special characters\n",
        "- Tokenization\n",
        "  - Split text into words or subwords\n",
        "  - spaCy or Stanford NLP tokenizers ensure linguistic accuracy\n",
        "- Lemmatization\n",
        "  - Convert words to base form (e.g., *feeling → feel*)\n",
        "- Stopword Handling\n",
        "  - Remove irrelevant words where appropriate\n",
        "- Sentence Segmentation\n",
        "  - Important for long user messages\n",
        "\n",
        "Role of NLP Libraries\n",
        "- spaCy: Fast tokenization, lemmatization, POS tagging, NER  \n",
        "- Stanford NLP: Deep linguistic analysis, dependency parsing, coreference resolution  \n",
        "\n",
        "These tools help extract meaningful linguistic features before modeling.\n",
        "\n",
        "\n",
        " 3. Role of LSTM / GRU Networks\n",
        "\n",
        "User conversations are sequential and depend heavily on context.  \n",
        "LSTM and GRU networks are ideal because they can model temporal dependencies in text.\n",
        "\n",
        "Why LSTM / GRU?\n",
        "- Maintain memory of previous words and sentences\n",
        "- Handle long-term dependencies\n",
        "- Reduce vanishing gradient issues seen in vanilla RNNs\n",
        "\n",
        "LSTM vs GRU\n",
        "- LSTM: Better for very long and complex conversations\n",
        "- GRU: Faster, simpler, and often sufficient for chatbot tasks\n",
        "\n",
        "\n",
        " 4. Model Architecture (High-Level)\n",
        "\n",
        "1. Embedding Layer\n",
        "   - Converts tokens into dense vectors\n",
        "   - Can use pretrained embeddings (GloVe, Word2Vec)\n",
        "2. LSTM / GRU Layer\n",
        "   - Captures context and emotional flow\n",
        "3. Dense Layers\n",
        "   - Used for intent classification or emotion detection\n",
        "4. Output Layer\n",
        "   - Predicts:\n",
        "     - User intent (e.g., anxiety, sadness, stress)\n",
        "     - Emotional state\n",
        "     - Appropriate response category\n",
        "\n",
        "\n",
        " 5. Understanding User Intent and Emotion\n",
        "\n",
        "The chatbot must identify:\n",
        "- Emotional state (sad, anxious, angry, neutral)\n",
        "- Intent (seeking help, venting, asking advice)\n",
        "\n",
        "# Techniques Used\n",
        "- LSTM/GRU-based emotion classification\n",
        "- Sentiment analysis as a supporting signal\n",
        "- Context tracking across multiple turns\n",
        "\n",
        "This helps the chatbot choose responses that are empathetic and relevant.\n",
        "\n",
        "\n",
        " 6. Response Generation Strategy\n",
        "\n",
        "Two main approaches can be used:\n",
        "\n",
        "1. Retrieval-Based Responses\n",
        "- Select the best response from a predefined, therapist-approved set\n",
        "- Safer and more controllable\n",
        "- Preferred for mental health applications\n",
        "\n",
        "2. Generative Responses\n",
        "- Use sequence-to-sequence LSTM/GRU models\n",
        "- Generates responses dynamically\n",
        "- Requires strong safety filters and monitoring\n",
        "\n",
        "In mental health platforms, retrieval-based systems are often preferred due to safety concerns.\n",
        "\n",
        "\n",
        " 7. Ethical and Safety Considerations\n",
        "\n",
        "Ethics are crucial in mental health chatbots.\n",
        "\n",
        " Key Ethical Concerns\n",
        "- User Privacy\n",
        "  - Encrypt conversations\n",
        "  - Comply with data protection laws\n",
        "- Emotional Safety\n",
        "  - Avoid harmful or dismissive responses\n",
        "  - Never replace professional medical advice\n",
        "- Bias and Fairness\n",
        "  - Ensure training data is diverse and unbiased\n",
        "- Crisis Handling\n",
        "  - Detect suicidal or self-harm intent\n",
        "  - Escalate to human professionals or emergency resources\n",
        "- Transparency\n",
        "  - Clearly inform users that the chatbot is not a human therapist\n",
        "\n",
        "\n",
        " 8. Continuous Improvement and Monitoring\n",
        "\n",
        "- Log anonymized interactions for improvement\n",
        "- Regularly retrain models with updated data\n",
        "- Human-in-the-loop review for critical cases\n",
        "- Monitor false positives and false negatives in emotion detection\n",
        "\n",
        "\n",
        "\n",
        "Conclusion\n",
        "\n",
        "By combining LSTM or GRU networks with powerful NLP tools like spaCy or Stanford NLP, a mental health chatbot can effectively understand user language, context, and emotions. However, success depends not only on technical accuracy but also on ethical design, safety mechanisms, and responsible deployment. Such systems should support users empathetically while encouraging professional help when necessary."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}