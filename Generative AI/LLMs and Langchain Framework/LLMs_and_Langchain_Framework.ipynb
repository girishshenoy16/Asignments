{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrP0jYxEfFBj"
      },
      "source": [
        "## LLMs and Langchain Framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_CqvK0ml_Ta"
      },
      "source": [
        "1) What are Large Language Models (LLMs) and how do they function?  \n",
        "\n",
        "->\n",
        "\n",
        "Large Language Models (LLMs) are advanced deep learning models trained on massive amounts of text data to understand, generate, and manipulate natural language. They are typically built using Transformer architectures, which rely heavily on self-attention mechanisms to process and learn contextual relationships between words.\n",
        "\n",
        " üîπ How LLMs Function\n",
        "\n",
        "1. Pretraining Phase\n",
        "   - LLMs are trained on large corpora (books, articles, websites, code).\n",
        "   - They learn language patterns by predicting the next word in a sequence.\n",
        "   - Objective function typically involves maximizing:\n",
        "     $$\n",
        "     P(w_t \\mid w_1, w_2, ..., w_{t-1})\n",
        "     $$\n",
        "\n",
        "2. Transformer Architecture\n",
        "   - Uses self-attention to weigh the importance of words relative to each other.\n",
        "   - Allows modeling of long-range dependencies in text.\n",
        "   - Eliminates recurrence used in older RNN-based systems.\n",
        "\n",
        "3. Fine-Tuning / Alignment\n",
        "   - Models are adapted for specific tasks.\n",
        "   - Techniques include supervised fine-tuning and Reinforcement Learning from Human Feedback (RLHF).\n",
        "\n",
        " üîπ Key Characteristics\n",
        "- Context-aware text generation\n",
        "- Few-shot and zero-shot learning\n",
        "- Ability to generalize across tasks\n",
        "\n",
        "In summary, LLMs function by learning statistical language representations and using attention mechanisms to generate contextually relevant outputs.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2) Discuss the impact of LLMs on traditional software development approaches.  \n",
        "\n",
        "->\n",
        "\n",
        "LLMs are significantly transforming traditional software development paradigms.\n",
        "\n",
        " üîπ 1. Shift from Rule-Based Systems to Prompt-Based Systems\n",
        "Traditional systems relied on:\n",
        "- Explicit logic\n",
        "- Deterministic workflows\n",
        "- Handcrafted NLP pipelines\n",
        "\n",
        "LLMs introduce:\n",
        "- Prompt engineering\n",
        "- Natural language interfaces\n",
        "- Flexible, adaptive responses\n",
        "\n",
        " üîπ 2. Code Generation & Automation\n",
        "- Developers can generate code using natural language prompts.\n",
        "- Automated debugging and documentation.\n",
        "- Rapid prototyping without deep boilerplate coding.\n",
        "\n",
        " üîπ 3. Reduced Development Time\n",
        "- Faster MVP creation.\n",
        "- Simplified API integration using AI copilots.\n",
        "\n",
        " üîπ 4. New Development Patterns\n",
        "- Rise of AI-first applications\n",
        "- Integration of LLM APIs instead of building models from scratch.\n",
        "- Use of retrieval-augmented generation (RAG) systems.\n",
        "\n",
        "Overall, LLMs shift development from writing detailed logic to designing intelligent prompts and workflows.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3) What are the key advantages and limitations of using LLMs in real-world applications?  \n",
        "\n",
        "->\n",
        "\n",
        " ‚úÖ Advantages\n",
        "\n",
        "1. Versatility\n",
        "   - Can handle multiple tasks (summarization, translation, coding).\n",
        "2. Contextual Understanding\n",
        "   - Generates human-like responses.\n",
        "3. Reduced Need for Labeled Data\n",
        "   - Few-shot learning capabilities.\n",
        "4. Scalability\n",
        "   - Can serve millions of users via APIs.\n",
        "5. Automation\n",
        "   - Reduces manual intervention in repetitive tasks.\n",
        "\n",
        "\n",
        "\n",
        " ‚ùå Limitations\n",
        "\n",
        "1. Hallucination\n",
        "   - May generate incorrect but confident answers.\n",
        "2. Bias\n",
        "   - Reflect biases present in training data.\n",
        "3. High Computational Cost\n",
        "   - Requires powerful hardware.\n",
        "4. Lack of True Understanding\n",
        "   - Operates on statistical patterns, not reasoning.\n",
        "5. Privacy Concerns\n",
        "   - Risk of sensitive data leakage.\n",
        "\n",
        "LLMs are powerful but must be deployed with monitoring and validation systems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4) Describe how different industries are being transformed by the use of LLMs. Provide examples.  \n",
        "\n",
        "->\n",
        "\n",
        "LLMs are reshaping industries by automating language-driven tasks.\n",
        "\n",
        " üîπ Healthcare\n",
        "- Medical report summarization\n",
        "- AI-assisted diagnosis suggestions\n",
        "- Clinical documentation automation\n",
        "\n",
        " üîπ Finance\n",
        "- Automated financial analysis\n",
        "- Fraud detection support\n",
        "- Customer service chatbots\n",
        "\n",
        " üîπ Education\n",
        "- Personalized tutoring systems\n",
        "- Automated grading\n",
        "- Content generation for learning materials\n",
        "\n",
        " üîπ Legal\n",
        "- Contract summarization\n",
        "- Case research assistance\n",
        "- Legal document drafting\n",
        "\n",
        " üîπ Software & Technology\n",
        "- Code generation\n",
        "- AI copilots\n",
        "- Documentation automation\n",
        "\n",
        " üîπ Media & Publishing\n",
        "- Article drafting\n",
        "- Scriptwriting\n",
        "- Creative content generation\n",
        "\n",
        "LLMs enhance efficiency, reduce costs, and enable personalization across sectors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "5) Compare and contrast LangChain and LlamaIndex. What unique problems does each solve?  \n",
        "\n",
        "->\n",
        "\n",
        "Both LangChain and LlamaIndex are frameworks for building LLM-powered applications, but they focus on different problems.\n",
        "\n",
        "\n",
        " üîπ LangChain\n",
        "\n",
        "Purpose:  \n",
        "Build complex LLM workflows and multi-step reasoning systems.\n",
        "\n",
        "Key Features:\n",
        "- Prompt chaining\n",
        "- Agents & tool usage\n",
        "- Memory management\n",
        "- API integrations\n",
        "- Multi-step decision workflows\n",
        "\n",
        "Use Case Example:\n",
        "- AI assistant that searches the web, queries databases, and responds intelligently.\n",
        "\n",
        " üîπ LlamaIndex\n",
        "\n",
        "Purpose:  \n",
        "Connect LLMs with external data sources efficiently.\n",
        "\n",
        "Key Features:\n",
        "- Document indexing\n",
        "- Retrieval-Augmented Generation (RAG)\n",
        "- Vector database integration\n",
        "- Structured data querying\n",
        "\n",
        "Use Case Example:\n",
        "- Question-answering system over company documents.\n",
        "\n",
        "\n",
        " üîπ Comparison Table\n",
        "\n",
        "| Feature | LangChain | LlamaIndex |\n",
        "|----------|------------|-------------|\n",
        "| Primary Focus | Workflow orchestration | Data retrieval & indexing |\n",
        "| Agents | Yes | Limited |\n",
        "| RAG Support | Yes | Core strength |\n",
        "| Best For | AI agents & pipelines | Document-based QA systems |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "lVWLgldlkdJY",
        "outputId": "215afa15-7575-4942-ae42-7ff47e6830f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.10)\n",
            "Collecting langchain-classic\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.10-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.13)\n",
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic)\n",
            "  Downloading langchain_text_splitters-1.1.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain-classic) (0.7.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic) (6.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic) (2.32.4)\n",
            "Requirement already satisfied: sqlalchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic) (2.0.46)\n",
            "Collecting packaging<25,>=23.2 (from langchainhub)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.4.20260107-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting requests<3.0.0,>=2.0.0 (from langchain-classic)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.4)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.13.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=2.20.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.21.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (0.3.6)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.8->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.17->langchain-classic) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=2.20.0->langchain-openai) (4.67.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->langchain-classic) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->langchain-classic) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->langchain-classic) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->langchain-classic) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3.0.0,>=1.4.0->langchain-classic) (3.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain-classic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.17->langchain-classic) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.8->langchain) (1.12.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.1.10-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_text_splitters-1.1.1-py3-none-any.whl (35 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20260107-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: types-requests, requests, packaging, mypy-extensions, typing-inspect, newsapi-python, marshmallow, langchainhub, dataclasses-json, langchain-text-splitters, langchain-openai, langchain-classic, langchain-community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 26.0\n",
            "    Uninstalling packaging-26.0:\n",
            "      Successfully uninstalled packaging-26.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-openai-1.1.10 langchain-text-splitters-1.1.1 langchainhub-0.1.21 marshmallow-3.26.2 mypy-extensions-1.1.0 newsapi-python-0.2.7 packaging-24.2 requests-2.32.5 types-requests-2.32.4.20260107 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              },
              "id": "1ee45af6a87b45c68d213f3665d6c964"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.14.15-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting llama-index-llms-openrouter\n",
            "  Downloading llama_index_llms_openrouter-0.4.4-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting llama-index-embeddings-huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl.metadata (458 bytes)\n",
            "Collecting llama-index-cli<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.5.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting llama-index-core<0.15.0,>=0.14.15 (from llama-index)\n",
            "  Downloading llama_index_core-0.14.15-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-index-llms-openai<0.7,>=0.6.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.6.19-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting llama-index-readers-file<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.5.6-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index) (3.9.1)\n",
            "Collecting llama-index-llms-openai-like<0.7,>=0.5.0 (from llama-index-llms-openrouter)\n",
            "  Downloading llama_index_llms_openai_like-0.6.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-huggingface) (5.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.24.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.3)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.15.0)\n",
            "\u001b[33mWARNING: huggingface-hub 1.4.1 does not provide the extra 'inference'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (3.13.3)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (0.22.1)\n",
            "Collecting banks<3,>=2.3.0 (from llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading banks-2.4.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (0.6.7)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting llama-index-workflows!=2.9.0,<3,>=2 (from llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading llama_index_workflows-2.14.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (3.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (4.9.2)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (2.12.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (2.32.5)\n",
            "Collecting setuptools>=80.9.0 (from llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading setuptools-82.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.15->llama-index) (2.0.46)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (9.1.4)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (0.12.0)\n",
            "Collecting tinytag>=2.2.0 (from llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading tinytag-2.2.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.15->llama-index) (2.1.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (2.21.0)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting wrapt (from llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2026.1.4)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
            "Requirement already satisfied: pandas<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<7,>=6.1.3 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
            "  Downloading pypdf-6.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.6,>=0.5.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.94-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (2025.11.3)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (5.0.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.10.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.16.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.15->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.15->llama-index) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.15->llama-index) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.15->llama-index) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.15->llama-index) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.15->llama-index) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.15->llama-index) (1.22.0)\n",
            "Collecting griffe (from banks<3,>=2.3.0->llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading griffe-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.3.0->llama-index-core<0.15.0,>=0.14.15->llama-index) (3.1.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.8.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.16.0)\n",
            "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting llama-cloud-services>=0.6.94 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.94-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.13.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.15->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.15->llama-index) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15.0,>=0.14.15->llama-index) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.15->llama-index) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15.0,>=0.14.15->llama-index) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.15->llama-index) (3.3.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.15->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.15->llama-index) (3.26.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.24.0)\n",
            "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.93-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting llama-cloud-services>=0.6.93 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.93-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.92-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.92 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.92-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.91-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-cloud-services>=0.6.91 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.91-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.90-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting llama-cloud-services>=0.6.90 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.90-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.89-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.89 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.89-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.88-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.88 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.88-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.87-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.87 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.87-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.86-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.86 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.86-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.85-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.85 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.85-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.84-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.84 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.84-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.83-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.82 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.83-py3-none-any.whl.metadata (3.3 kB)\n",
            "  Downloading llama_cloud_services-0.6.82-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.82-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.81-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.81 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.81-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.80-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.80 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.80-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.79-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.79 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.79-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.78-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.78 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.78-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.77-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.77 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.77-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.76-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.76 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.76-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.75-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.75 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.75-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.74-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.74 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.74-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.73-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.73 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.73-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.72-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.72 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.72-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.71-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.71 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.71-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.70-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.70 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.70-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.69-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.69 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.69-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.68-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.68 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.68-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.67-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.67 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.67-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.66-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.66 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.66-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.65-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.64 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.65-py3-none-any.whl.metadata (3.3 kB)\n",
            "  Downloading llama_cloud_services-0.6.64-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.64-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.63-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.63 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.63-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.62-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.62 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.62-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: python-dotenv<2,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.0.4)\n",
            "Collecting griffecli==2.0.0 (from griffe->banks<3,>=2.3.0->llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading griffecli-2.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting griffelib==2.0.0 (from griffe->banks<3,>=2.3.0->llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading griffelib-2.0.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting colorama>=0.4 (from griffecli==2.0.0->griffe->banks<3,>=2.3.0->llama-index-core<0.15.0,>=0.14.15->llama-index)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.3.0->llama-index-core<0.15.0,>=0.14.15->llama-index) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.1.2)\n",
            "Downloading llama_index-0.14.15-py3-none-any.whl (7.3 kB)\n",
            "Downloading llama_index_llms_openrouter-0.4.4-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl (8.9 kB)\n",
            "Downloading llama_index_cli-0.5.3-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.14.15-py3-none-any.whl (11.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.9.4-py3-none-any.whl (17 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.6.19-py3-none-any.whl (26 kB)\n",
            "Downloading llama_index_llms_openai_like-0.6.0-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_index_readers_file-0.5.6-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.5.1-py3-none-any.whl (3.2 kB)\n",
            "Downloading banks-2.4.1-py3-none-any.whl (35 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_index_workflows-2.14.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.7.2-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m331.2/331.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-82.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tinytag-2.2.0-py3-none-any.whl (32 kB)\n",
            "Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\n",
            "Downloading griffe-2.0.0-py3-none-any.whl (5.2 kB)\n",
            "Downloading griffecli-2.0.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading griffelib-2.0.0-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, wrapt, tinytag, setuptools, pypdf, griffelib, colorama, griffecli, deprecated, llama-index-instrumentation, llama-cloud, griffe, llama-index-workflows, banks, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-llms-openai-like, llama-index-cli, llama-index-readers-llama-parse, llama-index-llms-openrouter, llama-index-embeddings-huggingface, llama-index\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 2.1.1\n",
            "    Uninstalling wrapt-2.1.1:\n",
            "      Successfully uninstalled wrapt-2.1.1\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed banks-2.4.1 colorama-0.4.6 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 griffe-2.0.0 griffecli-2.0.0 griffelib-2.0.0 llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-0.14.15 llama-index-cli-0.5.3 llama-index-core-0.14.15 llama-index-embeddings-huggingface-0.6.1 llama-index-embeddings-openai-0.5.1 llama-index-indices-managed-llama-cloud-0.9.4 llama-index-instrumentation-0.4.2 llama-index-llms-openai-0.6.19 llama-index-llms-openai-like-0.6.0 llama-index-llms-openrouter-0.4.4 llama-index-readers-file-0.5.6 llama-index-readers-llama-parse-0.5.1 llama-index-workflows-2.14.2 llama-parse-0.6.54 pypdf-6.7.2 setuptools-82.0.0 striprtf-0.0.26 tinytag-2.2.0 wrapt-1.17.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "235be6737dfc42f9b6eb7c315b408a4a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Installing required packages\n",
        "\n",
        "%pip install langchain langchain-classic langchainhub langchain-community langchain-openai langchain-core newsapi-python\n",
        "\n",
        "%pip install llama-index llama-index-llms-openrouter llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "tz8e8PrBRRkO",
        "outputId": "8d880d20-b1b7-4d52-b793-60682a1ded11"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# AI Architectural Analysis\n\n## 1Ô∏è‚É£ Architectural Trade‚Äëoffs: **LangGraph‚Äëbased Multi‚ÄëAgent System (MAS)** vs **Standard Linear Sequential Chain**\n\n| Aspect | **LangGraph (MAS)** | **Linear Sequential Chain** | **Key Take‚Äëaways** |\n|--------|--------------------|----------------------------|--------------------|\n| **Latency** | ‚Ä¢‚ÄØEach agent runs **concurrently** (or in a controlled async pipeline). <br>‚Ä¢‚ÄØInter‚Äëagent messaging adds **network/IPC overhead** but can be hidden behind parallelism. <br>‚Ä¢‚ÄØBest‚Äëcase: **O(max‚ÄØagent‚ÄØtime)**, worst‚Äëcase: **O(Œ£‚ÄØagent‚ÄØtime + messaging)**. | ‚Ä¢‚ÄØStrict **step‚Äëby‚Äëstep** execution. <br>‚Ä¢‚ÄØLatency = Œ£‚ÄØ(step‚ÄØtime). <br>‚Ä¢‚ÄØNo messaging overhead, but no parallelism. | - MAS shines when agents are **independent** or can be pipelined. <br>- Linear chain is predictable but slower for heavy, independent sub‚Äëtasks. |\n| **Complexity** | ‚Ä¢‚ÄØRequires **graph orchestration**, state‚Äëmanagement, and **routing logic** (e.g., role‚Äëbased dispatch, dynamic branching). <br>‚Ä¢‚ÄØDebugging is **non‚Äëlinear** (multiple paths, cycles). <br>‚Ä¢‚ÄØHigher learning curve for developers unfamiliar with graph APIs. | ‚Ä¢‚ÄØSimple **procedural code** ‚Äì one function after another. <br>‚Ä¢‚ÄØEasier to reason about, test, and version. | - MAS adds **architectural overhead** but pays off for modularity and reuse. <br>- Linear chain is low‚Äëmaintenance for straightforward pipelines. |\n| **Reliability** | ‚Ä¢‚ÄØFault isolation: a failing agent can be **re‚Äëtried** or **re‚Äërouted** without aborting the whole graph. <br>‚Ä¢‚ÄØSupports **circuit‚Äëbreaker**, **timeout**, and **fallback** per node. <br>‚Ä¢‚ÄØPotential for **dead‚Äëlocks** or **livelocks** in cyclic graphs if not guarded. | ‚Ä¢‚ÄØFailure in any step aborts the whole chain (unless explicit try/catch). <br>‚Ä¢‚ÄØSimpler error surface, but less graceful degradation. | - MAS offers **higher resilience** via per‚Äënode policies, at the cost of needing careful cycle handling. <br>- Linear chain is **simpler but brittle**. |\n\n> **TL;DR** ‚Äì Choose **LangGraph** when you need **parallelism, modularity, and fine‚Äëgrained fault tolerance**; stick with a **linear chain** for simple, deterministic workflows where latency predictability and low engineering overhead are paramount.\n\n---\n\n## 2Ô∏è‚É£ When a **120‚ÄëBillion‚Äëparameter** Model is Mandatory Over a **7‚ÄëBillion‚Äëparameter** Model\n\n| Situation | Why 120‚ÄØB is Needed | What 7‚ÄØB Lacks |\n|-----------|--------------------|----------------|\n| **Domain‚Äëspecific, high‚Äëprecision scientific reasoning** (e.g., quantum chemistry, climate‚Äëmodel parameter inference) | ‚Ä¢‚ÄØThe 120‚ÄØB model has **richer internal representations** and can capture subtle physical relationships that only emerge after training on massive multi‚Äëmodal corpora. <br>‚Ä¢‚ÄØHigher capacity enables **few‚Äëshot** extrapolation to unseen equations or rare phenomena. | ‚Ä¢‚ÄØ7‚ÄØB struggles with **out‚Äëof‚Äëdistribution** scientific jargon and cannot reliably infer complex equations without extensive fine‚Äëtuning. |\n| **Legal contract analysis for multinational, multi‚Äëjurisdictional agreements** | ‚Ä¢‚ÄØLarge models encode **broader legal precedent** across many jurisdictions, reducing hallucination risk. <br>‚Ä¢‚ÄØThey can **disambiguate nuanced clause interactions** (e.g., force‚Äëmajeure vs. termination rights) with higher fidelity. | ‚Ä¢‚ÄØ7‚ÄØB often **confuses similar legal terms** and may miss cross‚Äëclause dependencies, leading to costly errors. |\n| **Creative generation requiring deep world‚Äëbuilding** (e.g., novel‚Äëlength interactive fiction with consistent lore) | ‚Ä¢‚ÄØ120‚ÄØB maintains **long‚Äërange coherence** over thousands of tokens, preserving character arcs, plot threads, and world rules. <br>‚Ä¢‚ÄØIt can **reuse rare entities** introduced early without explicit memory hacks. | ‚Ä¢‚ÄØ7‚ÄØB quickly loses context, repeats or contradicts earlier plot points, requiring heavy external memory scaffolding. |\n| **Zero‚Äëshot multilingual translation for low‚Äëresource languages** | ‚Ä¢‚ÄØThe massive model has **exposed more language pairs** during pre‚Äëtraining, yielding better zero‚Äëshot performance. | ‚Ä¢‚ÄØ7‚ÄØB exhibits **high BLEU variance** and often fails to preserve meaning for low‚Äëresource scripts. |\n\n> **Bottom line:** Deploy a **120‚ÄØB** model when **accuracy, domain depth, or long‚Äëcontext coherence** are non‚Äënegotiable and the cost (GPU memory, inference latency, cloud spend) can be justified.\n\n---\n\n## 3Ô∏è‚É£ Strategy for **Error Handling in Cyclic Graphs** (LangGraph)\n\nA cyclic (feedback) sub‚Äëgraph is powerful for iterative refinement (e.g., self‚Äëcritique loops) but can easily become a runaway. Below is a concise, reusable pattern:\n\n```mermaid\ngraph TD\n    A[Start Node] --> B[Processing Agent]\n    B --> C{Convergence?}\n    C -- Yes --> D[Output]\n    C -- No --> B\n    C -- Error --> E[Error Handler]\n    E --> F[Back‚Äëoff / Abort]\n```\n\n### Step‚Äëby‚ÄëStep Blueprint\n\n| Phase | Action | Rationale |\n|------|--------|-----------|\n| **1Ô∏è‚É£ Guarded Loop Condition** | - Each iteration returns a **status flag** (`converged`, `continue`, `error`). <br>- Use a **max‚Äëiteration ceiling** (e.g., `max_iters = 5`). | Prevents infinite loops and provides a deterministic exit point. |\n| **2Ô∏è‚É£ Per‚ÄëNode Timeouts & Retries** | - Wrap every agent call in a **circuit‚Äëbreaker** with a timeout (e.g., 2‚ÄØs). <br>- On timeout, **retry N times** with exponential back‚Äëoff before bubbling an error. | Isolates flaky LLM calls; avoids stalling the whole cycle. |\n| **3Ô∏è‚É£ Centralized Error Node** | - Funnel all non‚Äërecoverable errors to a dedicated **ErrorHandler** node. <br>- The node decides: *abort*, *fallback to a cheaper model*, or *log & continue with degraded output*. | Guarantees a single place to enforce policy, simplifying observability. |\n| **4Ô∏è‚É£ State Snapshot & Rollback** | - Persist the **graph state** (agent inputs/outputs) at each iteration. <br>- If an error is deemed unrecoverable, **rollback** to the last known‚Äëgood snapshot. | Enables graceful recovery without losing prior progress. |\n| **5Ô∏è‚É£ Observability Hooks** | - Emit **metrics**: `iteration_count`, `error_rate`, `latency_per_iter`. <br>- Attach **tracing IDs** to correlate logs across cycles. | Makes it possible to detect pathological loops in production. |\n| **6Ô∏è‚É£ Adaptive Loop Termination** | - Combine **hard caps** (`max_iters`) with **soft convergence** (e.g., cosine similarity > 0.95 between successive outputs). <br>- If soft criteria are met early, break out early. | Saves compute while still guaranteeing quality. |\n\n#### Pseudocode Sketch (Python‚Äëlike)\n\n```python\ndef run_cyclic_graph(input_payload):\n    state = {\"prev_output\": None}\n    for i in range(MAX_ITERS):\n        try:\n            out = agent.process(state[\"prev_output\"] or input_payload,\n                                timeout=TIMEOUT_SEC)\n        except TimeoutError:\n            if i < RETRY_LIMIT:\n                continue  # retry same iteration\n            else:\n                return error_handler(\"timeout\", state)\n\n        if converged(state[\"prev_output\"], out):\n            return out  # success\n\n        state[\"prev_output\"] = out\n\n    # Max iterations reached without convergence\n    return error_handler(\"max_iters_exceeded\", state)\n```\n\n---\n\n## üìö TL;DR Summary\n\n| Decision | Use **LangGraph MAS** | Use **Linear Chain** |\n|----------|----------------------|----------------------|\n| **Parallel independent work** | ‚úÖ | ‚ùå |\n| **Simple, one‚Äëoff transformation** | ‚ùå (over‚Äëengineered) | ‚úÖ |\n| **Fine‚Äëgrained fault isolation** | ‚úÖ | ‚ùå |\n| **Predictable latency & low dev cost** | ‚ùå | ‚úÖ |\n| **Need for iterative self‚Äëcritique loops** | ‚úÖ (with robust error handling) | ‚ùå |\n\nWhen the problem domain demands **deep reasoning, massive context, or high‚Äëstakes accuracy**, a **120‚ÄØB** model is justified despite its cost. For cyclic MAS, enforce **timeouts, iteration caps, and a centralized error node** to keep the system reliable and observable."
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "6) Implement a basic Langchain pipeline using OpenAI‚Äôs LLM to answer questions based on a user input prompt.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, Markdown\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Securely fetch API Key from Colab Secrets\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n",
        "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "# Initialize the High-Capacity Model\n",
        "llm = ChatOpenAI(\n",
        "\n",
        "    # Using your specified 120B model\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    openai_api_key=OPENROUTER_API_KEY,\n",
        "    openai_api_base=OPENROUTER_BASE_URL,\n",
        "    default_headers={\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com\",\n",
        "        \"X-Title\": \"Architectural Analysis Tool\",\n",
        "    },\n",
        "\n",
        "    # Lower temperature for more structured, analytical responses\n",
        "    temperature=0.4\n",
        ")\n",
        "\n",
        "# Define the Pipeline with a \"Suitable\" Professional Question\n",
        "# Use a System Message to set the persona for high-quality output\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a Senior Principal Software Architect. Provide responses using Markdown with clear headers, tables for comparison, and Mermaid.js diagram descriptions if helpful.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# A Suitable \"Stress Test\" Question for a 120B Model\n",
        "# This asks for architectural trade-offs, which requires deep reasoning.\n",
        "complex_question = \"\"\"\n",
        "Explain the architectural trade-offs between implementing a Multi-Agent System (MAS)\n",
        "using LangGraph versus a standard linear sequential Chain.\n",
        "\n",
        "Please provide:\n",
        "1. A comparison table (Latency, Complexity, Reliability).\n",
        "2. A scenario where a 120B parameter model is specifically required over a 7B model.\n",
        "3. A brief strategy for error handling in cyclic graphs.\n",
        "\"\"\"\n",
        "\n",
        "# Run and Render\n",
        "response = chain.invoke({\"input\": complex_question})\n",
        "\n",
        "# This \"pastes\" the response into a formatted Markdown view\n",
        "display(Markdown(f\"# AI Architectural Analysis\\n\\n{response}\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "vKYRM7camlV4",
        "outputId": "ac289465-1184-432a-c48f-849a34650fda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `search_news` with `{'query': 'OpenAI latest news today'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m- OpenAI‚Äôs president is a Trump mega-donor (The Verge)\n",
            "- OpenAI should build Slack (Latent.space)\n",
            "- Microsoft CFO's memo to staff calls out AI deals, coding, and chips (Business Insider)\u001b[0m\u001b[32;1m\u001b[1;3mHere are the most recent headlines about OpenAI that were published today (February‚ÄØ22‚ÄØ2026), along with a short summary of each story:\n",
            "\n",
            "| Source | Headline | Summary |\n",
            "|--------|----------|---------|\n",
            "| **The Verge** | **‚ÄúOpenAI‚Äôs president is a Trump mega‚Äëdonor‚Äù** | An investigative report reveals that OpenAI‚Äôs president, **Brad Lightcap**, has a history of large political contributions to former President Donald Trump‚Äôs campaigns and affiliated political action committees. The article examines how Lightcap‚Äôs political ties intersect with OpenAI‚Äôs growing influence in the AI industry and raises questions about potential policy implications. |\n",
            "| **Latent.space** | **‚ÄúOpenAI should build Slack‚Äù** | A commentary piece argues that OpenAI could expand its product ecosystem by creating a Slack‚Äëlike collaboration platform that integrates its conversational models (ChatGPT, GPT‚Äë4o, etc.) directly into workplace messaging. The author outlines possible features‚Äîreal‚Äëtime AI assistance, automated summarization, and secure data handling‚Äîand suggests how such a product could compete with existing enterprise chat tools. |\n",
            "| **Business Insider** | **‚ÄúMicrosoft CFO‚Äôs memo to staff calls out AI deals, coding, and chips‚Äù** | Microsoft‚Äôs chief financial officer, **Amy Hood**, sent an internal memo highlighting the company‚Äôs strategic focus on AI partnerships, including the deepening relationship with OpenAI. The memo emphasizes upcoming AI‚Äërelated deals, the importance of AI‚Äëassisted coding tools (like GitHub Copilot), and the need for advanced semiconductor capacity to support large‚Äëscale model training‚Äîareas where OpenAI‚Äôs technology is a central driver. |\n",
            "\n",
            "**Key takeaways**\n",
            "\n",
            "1. **Political scrutiny:** OpenAI‚Äôs leadership is now under media spotlight for past political donations, which could affect public perception and regulatory attention.\n",
            "2. **Product expansion ideas:** Analysts are already speculating about new OpenAI‚Äëpowered collaboration tools that could reshape enterprise communication.\n",
            "3. **Strategic partnership with Microsoft:** The memo from Microsoft‚Äôs CFO underscores how critical OpenAI‚Äôs models are to Microsoft‚Äôs AI roadmap, especially regarding cloud infrastructure, AI‚Äëenhanced developer tools, and hardware investments.\n",
            "\n",
            "These stories together illustrate the multifaceted attention OpenAI is receiving today‚Äîfrom political and governance concerns, to product‚Äëstrategy speculation, to its pivotal role in the broader AI ecosystem through its partnership with Microsoft.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the latest news about OpenAI today?',\n",
              " 'output': 'Here are the most recent headlines about OpenAI that were published today (February\\u202f22\\u202f2026), along with a short summary of each story:\\n\\n| Source | Headline | Summary |\\n|--------|----------|---------|\\n| **The Verge** | **‚ÄúOpenAI‚Äôs president is a Trump mega‚Äëdonor‚Äù** | An investigative report reveals that OpenAI‚Äôs president, **Brad Lightcap**, has a history of large political contributions to former President Donald Trump‚Äôs campaigns and affiliated political action committees. The article examines how Lightcap‚Äôs political ties intersect with OpenAI‚Äôs growing influence in the AI industry and raises questions about potential policy implications. |\\n| **Latent.space** | **‚ÄúOpenAI should build Slack‚Äù** | A commentary piece argues that OpenAI could expand its product ecosystem by creating a Slack‚Äëlike collaboration platform that integrates its conversational models (ChatGPT, GPT‚Äë4o, etc.) directly into workplace messaging. The author outlines possible features‚Äîreal‚Äëtime AI assistance, automated summarization, and secure data handling‚Äîand suggests how such a product could compete with existing enterprise chat tools. |\\n| **Business Insider** | **‚ÄúMicrosoft CFO‚Äôs memo to staff calls out AI deals, coding, and chips‚Äù** | Microsoft‚Äôs chief financial officer, **Amy Hood**, sent an internal memo highlighting the company‚Äôs strategic focus on AI partnerships, including the deepening relationship with OpenAI. The memo emphasizes upcoming AI‚Äërelated deals, the importance of AI‚Äëassisted coding tools (like GitHub Copilot), and the need for advanced semiconductor capacity to support large‚Äëscale model training‚Äîareas where OpenAI‚Äôs technology is a central driver. |\\n\\n**Key takeaways**\\n\\n1. **Political scrutiny:** OpenAI‚Äôs leadership is now under media spotlight for past political donations, which could affect public perception and regulatory attention.\\n2. **Product expansion ideas:** Analysts are already speculating about new OpenAI‚Äëpowered collaboration tools that could reshape enterprise communication.\\n3. **Strategic partnership with Microsoft:** The memo from Microsoft‚Äôs CFO underscores how critical OpenAI‚Äôs models are to Microsoft‚Äôs AI roadmap, especially regarding cloud infrastructure, AI‚Äëenhanced developer tools, and hardware investments.\\n\\nThese stories together illustrate the multifaceted attention OpenAI is receiving today‚Äîfrom political and governance concerns, to product‚Äëstrategy speculation, to its pivotal role in the broader AI ecosystem through its partnership with Microsoft.'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"\n",
        "7)  Integrate Langchain with a third-party API (e.g., weather, news) and show how responses can be generated via LLMs.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langchain_classic.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain_classic import hub\n",
        "\n",
        "# Setup (OpenRouter & NewsAPI)\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENROUTER_API_KEY')\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
        "NEWS_API_KEY = userdata.get('NEWS_API_KEY')\n",
        "\n",
        "# Define Tool\n",
        "@tool\n",
        "def search_news(query: str):\n",
        "    \"\"\"Searches for current news articles.\"\"\"\n",
        "    url = f\"https://newsapi.org/v2/everything?q={query}&apiKey={NEWS_API_KEY}&pageSize=3\"\n",
        "    try:\n",
        "        data = requests.get(url).json()\n",
        "        articles = data.get(\"articles\", [])\n",
        "        return \"\\n\".join([f\"- {a['title']} ({a['source']['name']})\" for a in articles])\n",
        "    except:\n",
        "        return \"News search failed.\"\n",
        "\n",
        "# Initialize Model\n",
        "llm = ChatOpenAI(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    temperature=0,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\"\n",
        ")\n",
        "\n",
        "# Create the Agent\n",
        "tools = [search_news]\n",
        "\n",
        "# Pull a prompt specifically designed for tool-calling\n",
        "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "\n",
        "# Create the agent using the 'classic' constructor\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "\n",
        "# Execute\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "agent_executor.invoke({\"input\": \"What is the latest news about OpenAI today?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450,
          "referenced_widgets": [
            "c7123874a2cb43538b6d912bff9642e9",
            "f2974963ef894ddda80da8af2742abec",
            "076aac1d9476474d8a70539dfa133f5f",
            "eafc27a883c74775965f93b9746d2d28",
            "c44ae18058d2489598a1354a23a0d6ef",
            "a28ca8d973b24ed6bea3c8b056afc913",
            "d1430e604616405da70c3bf289011f8c",
            "092d36b7092d44cb82e26cdb48356e25",
            "5281f4f93c264337a75764f52d3bd080",
            "7efb89be1b9b46eda99a479cc8ba575e",
            "b18bc16e8e38437f9f28d0fb65a09f74"
          ]
        },
        "collapsed": true,
        "id": "6MbiezoHmsjw",
        "outputId": "d744edd6-b50e-4d3e-85bb-7044dc930663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ File 'my_knowledge_base.txt' created and updated with Virat Kohli facts.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7123874a2cb43538b6d912bff9642e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: BAAI/bge-small-en-v1.5\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: List three major records held by Virat Kohli mentioned in the text.\n",
            "Answer: - Holds the record for the most centuries in One‚ÄëDay International cricket, overtaking Sachin‚ÄØTendulkar.  \n",
            "- Became the fastest player ever to reach 10,000, then 11,000 and subsequently 12,000 runs in ODIs.  \n",
            "- Is the all‚Äëtime leading run‚Äëscorer in the Indian Premier League.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "8) Create a LamaIndex implementation that indexes a local text file and retrieves answers from it.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.llms.openrouter import OpenRouter\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# Create/Update Knowledge Base with 30 lines on Virat Kohli\n",
        "\n",
        "kohli_facts = [\n",
        "    \"Virat Kohli was born on November 5, 1988, in Delhi, India.\\n\",\n",
        "    \"He is widely regarded as one of the greatest batsmen in the history of cricket.\\n\",\n",
        "    \"He led the Indian team to victory in the 2008 Under-19 World Cup in Malaysia.\\n\",\n",
        "    \"Kohli made his international debut for India in an ODI against Sri Lanka in 2008.\\n\",\n",
        "    \"He is known by the nickname 'Chiku,' given to him by his coach Ajit Chaudhary.\\n\",\n",
        "    \"In 2013, he reached the number one spot in the ICC rankings for ODI batsmen for the first time.\\n\",\n",
        "    \"He holds the record for the most centuries in ODI cricket, surpassing Sachin Tendulkar.\\n\",\n",
        "    \"Kohli was the fastest player to reach 10,000, 11,000, and 12,000 runs in ODI cricket.\\n\",\n",
        "    \"He took over the Test captaincy of the Indian team in 2014 after MS Dhoni's retirement.\\n\",\n",
        "    \"Under his captaincy, India became the number one ranked Test team in the world.\\n\",\n",
        "    \"He led India to its first-ever Test series win on Australian soil in 2018-19.\\n\",\n",
        "    \"Kohli has won the ICC ODI Player of the Year award multiple times.\\n\",\n",
        "    \"He was awarded the Rajiv Gandhi Khel Ratna, India's highest sporting honor, in 2018.\\n\",\n",
        "    \"In 2017, he was awarded the Padma Shri, India's fourth-highest civilian award.\\n\",\n",
        "    \"He is the highest run-scorer in the history of the Indian Premier League (IPL).\\n\",\n",
        "    \"Kohli played for the Royal Challengers Bangalore (RCB) since the inception of the IPL in 2008.\\n\",\n",
        "    \"He scored four centuries in a single IPL season in 2016, a record at the time.\\n\",\n",
        "    \"He is known for his incredible fitness levels and transformation of the Indian team's culture.\\n\",\n",
        "    \"Virat is an aggressive right-handed top-order batsman known for his cover drive.\\n\",\n",
        "    \"He married Bollywood actress Anushka Sharma in December 2017 in Italy.\\n\",\n",
        "    \"Kohli has a massive social media following, being one of the most followed athletes globally.\\n\",\n",
        "    \"He stepped down as India's T20I captain after the 2021 T20 World Cup.\\n\",\n",
        "    \"Shortly after, he also stepped down as the Test and ODI captain of the national side.\\n\",\n",
        "    \"He was part of the Indian squad that won the 2011 ICC Cricket World Cup.\\n\",\n",
        "    \"Kohli won the Man of the Tournament award in the 2014 and 2016 ICC World T20.\\n\",\n",
        "    \"He is the first player to score 50 centuries in One Day Internationals.\\n\",\n",
        "    \"His father, Prem Kohli, was a criminal lawyer who supported his early cricket career.\\n\",\n",
        "    \"Virat has his own fashion brand called Wrogn and a chain of gyms called Chisel.\\n\",\n",
        "    \"He is deeply involved in philanthropy through the Virat Kohli Foundation (VKF).\\n\",\n",
        "    \"In 2024, he played a crucial role in India winning the ICC T20 World Cup.\\n\"\n",
        "]\n",
        "\n",
        "# Write initial content and append Kohli facts\n",
        "with open(\"my_knowledge_base.txt\", \"a\") as f:\n",
        "    f.writelines(kohli_facts)\n",
        "\n",
        "print(\"‚úÖ File 'my_knowledge_base.txt' created and updated with Virat Kohli facts.\")\n",
        "\n",
        "# Setup Credentials & Models ---\n",
        "OR_KEY = userdata.get('OPENROUTER_API_KEY')\n",
        "\n",
        "# Initialize Model\n",
        "Settings.llm = OpenRouter(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    api_key=OR_KEY,\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "# Local embeddings remain free\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# Load, Index, and Query ---\n",
        "try:\n",
        "    # Load the document we just created\n",
        "    documents = SimpleDirectoryReader(input_files=[\"my_knowledge_base.txt\"]).load_data()\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    query_engine = index.as_query_engine()\n",
        "\n",
        "    # Run a test query about the new data\n",
        "    question = \"List three major records held by Virat Kohli mentioned in the text.\"\n",
        "    response = query_engine.query(question)\n",
        "\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "    print(f\"Answer: {response}\")\n",
        "\n",
        "except Exception as e:\n",
        "    if \"404\" in str(e):\n",
        "        print(\"\\n‚ùå ERROR: OpenRouter blocked the request.\")\n",
        "    else:\n",
        "        print(f\"\\nAn error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVHIfAZRklEA",
        "outputId": "fb8f6d00-1f91-47c4-986c-c593b045ce20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading the research paper...\n",
            "‚úÖ Success: PDF downloaded and verified.\n",
            "\n",
            "Analyzing: What does the paper state about the impact of AI on the healthcare industry and patient outcomes?\n",
            "\n",
            "--- AI RESEARCH ANALYSIS ---\n",
            "The paper argues that AI is reshaping the health‚Äëcare sector in two major ways:\n",
            "\n",
            "1. **Industry‚Äëwide transformation** ‚Äì Over the last five‚Äëto‚Äëten years AI has moved from a niche research tool to a core technology that is ‚Äúincreasingly valuable‚Äù to health‚Äëcare organizations. By automating and speeding up diagnostic workflows, AI is expected to overhaul traditional practice patterns, cut costs, and improve operational efficiency across the whole system.\n",
            "\n",
            "2. **Better patient outcomes** ‚Äì Because AI‚Äëdriven tools can **process data faster and more accurately** than clinicians working alone, diagnoses are delivered more quickly and with fewer errors. In addition, AI‚Äëbased monitoring systems can spot early signs of disease or clinical deterioration, enabling earlier intervention. These improvements‚Äîfaster, more precise assessments and earlier detection‚Äîare projected to lead to **improved treatment effectiveness, reduced complications, and overall better health outcomes for patients**.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "9) Demonstrate combining Langchain with LamaIndex to create a simple document-based Q&A chatbot.\n",
        "\n",
        "->\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.documents import Document\n",
        "from pydantic import Field\n",
        "from typing import List\n",
        "\n",
        "# Config & Secure Download\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n",
        "FILENAME = \"ai_applications.pdf\"\n",
        "\n",
        "# Your provided link (Direct PDF URL)\n",
        "DOCUMENT_URL = \"https://ijrti.org/papers/IJRTI2304061.pdf\"\n",
        "\n",
        "print(\"Downloading the research paper...\")\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "response = requests.get(DOCUMENT_URL, headers=headers, timeout=30)\n",
        "\n",
        "if response.status_code == 200 and b'%PDF' in response.content[:10]:\n",
        "    with open(FILENAME, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    print(\"‚úÖ Success: PDF downloaded and verified.\")\n",
        "else:\n",
        "    print(f\"‚ùå Error: Could not download PDF. Status: {response.status_code}\")\n",
        "    raise Exception(\"Check the URL or your network connection.\")\n",
        "\n",
        "# LlamaIndex Setup (The Storage Specialist)\n",
        "documents = SimpleDirectoryReader(input_files=[FILENAME]).load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Modern Bridge to stop the 'AttributeError'\n",
        "# This tells LangChain to use LlamaIndex's modern QueryEngine correctly\n",
        "class LlamaIndexBridge(BaseRetriever):\n",
        "    index: VectorStoreIndex = Field(exclude=True)\n",
        "\n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        # Using LlamaIndex's modern QueryEngine\n",
        "        query_engine = self.index.as_query_engine(similarity_top_k=3)\n",
        "        response = query_engine.query(query)\n",
        "        # Convert LlamaIndex result to LangChain format\n",
        "        return [Document(page_content=str(response))]\n",
        "\n",
        "# Setup LangChain LLM (The Orchestrator)\n",
        "llm = ChatOpenAI(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    openai_api_key=OPENROUTER_API_KEY,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    default_headers={\"HTTP-Referer\": \"https://colab.research.google.com\", \"X-Title\": \"Research-Analyst-Bot\"}\n",
        ")\n",
        "\n",
        "# Build the Final QA Chain\n",
        "custom_retriever = LlamaIndexBridge(index=index)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=custom_retriever\n",
        ")\n",
        "\n",
        "# Technical Query\n",
        "query = \"What does the paper state about the impact of AI on the healthcare industry and patient outcomes?\"\n",
        "print(f\"\\nAnalyzing: {query}\")\n",
        "result = qa_chain.invoke(query)\n",
        "\n",
        "print(f\"\\n--- AI RESEARCH ANALYSIS ---\\n{result['result']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDY7bDvfeVmv"
      },
      "source": [
        "10) A legal firm wants to use AI to summarize large volumes of legal documents and retrieve relevant information quickly. Propose a solution using Langchain and LamaIndex, and explain how it would work in practice.\n",
        "\n",
        "->\n",
        "\n",
        "To help a legal firm efficiently summarize and retrieve information from large volumes of legal documents, we can design a Retrieval-Augmented Generation (RAG) system that combines the strengths of LlamaIndex (for document indexing & retrieval) and LangChain (for orchestration & workflow management).\n",
        "\n",
        "This solution enables:\n",
        "- Fast document search  \n",
        "- Accurate summarization  \n",
        "- Context-aware question answering  \n",
        "- Scalable document management  \n",
        "\n",
        "\n",
        "üîπ Overall System Architecture\n",
        "\n",
        "The proposed system consists of the following components:\n",
        "\n",
        "1. Document Ingestion Layer\n",
        "2. Indexing & Embedding Layer (LlamaIndex)\n",
        "3. Retrieval Layer (Vector Search)\n",
        "4. LLM Processing Layer (Summarization & QA)\n",
        "5. Workflow Orchestration (LangChain)\n",
        "6. User Interface (Legal Dashboard)\n",
        "\n",
        "\n",
        "üîπ Step 1: Document Ingestion & Preprocessing\n",
        "\n",
        "The legal firm uploads documents such as:\n",
        "- Contracts\n",
        "- Case files\n",
        "- Court judgments\n",
        "- Agreements\n",
        "- Regulatory documents\n",
        "\n",
        "\n",
        "Preprocessing Steps:\n",
        "- Convert PDFs / Word files into text\n",
        "- Chunk documents into smaller sections (e.g., 500‚Äì1000 tokens)\n",
        "- Remove unnecessary formatting\n",
        "- Store metadata (case number, client name, date)\n",
        "\n",
        "This ensures the system can handle very large legal documents efficiently.\n",
        "\n",
        "\n",
        "üîπ Step 2: Indexing with LlamaIndex\n",
        "\n",
        "LlamaIndex is used to:\n",
        "- Convert document chunks into embeddings\n",
        "- Store them in a vector database (e.g., FAISS, Pinecone, Chroma)\n",
        "\n",
        "How it Works:\n",
        "Each chunk is transformed into a numerical vector representation:\n",
        "\n",
        "$$\n",
        "\\text{Embedding} = f(\\text{Document Chunk})\n",
        "$$\n",
        "\n",
        "These embeddings allow semantic similarity search instead of keyword matching.\n",
        "\n",
        "Benefits:\n",
        "- Fast retrieval of relevant sections\n",
        "- Semantic understanding of legal language\n",
        "- Efficient search across thousands of documents\n",
        "\n",
        "\n",
        "üîπ Step 3: Retrieval-Augmented Generation (RAG)\n",
        "\n",
        "When a lawyer asks a question:\n",
        "\n",
        "> \"Summarize the termination clause in Contract A.\"\n",
        "\n",
        "The system:\n",
        "1. Converts the query into an embedding.\n",
        "2. Finds the most relevant document chunks via vector similarity search.\n",
        "3. Passes retrieved content to the LLM.\n",
        "\n",
        "This prevents hallucination and ensures responses are grounded in actual documents.\n",
        "\n",
        "\n",
        "üîπ Step 4: Summarization Using LLM\n",
        "\n",
        "The LLM (e.g., GPT-style model) receives:\n",
        "\n",
        "- Retrieved legal text\n",
        "- Clear summarization prompt\n",
        "\n",
        "Example Prompt:\n",
        "```\n",
        "\n",
        "Summarize the following legal clause in simple terms:\n",
        "[Retrieved Clause Text]\n",
        "\n",
        "```\n",
        "\n",
        "The LLM generates:\n",
        "- Concise summaries\n",
        "- Risk highlights\n",
        "- Key legal obligations\n",
        "\n",
        "\n",
        "üîπ Step 5: Workflow Orchestration with LangChain\n",
        "\n",
        "LangChain manages complex workflows such as:\n",
        "\n",
        "- Multi-step summarization\n",
        "- Chain-of-thought reasoning\n",
        "- Tool usage (e.g., citation lookup, clause comparison)\n",
        "- Conversation memory for follow-up queries\n",
        "\n",
        "Example Workflow:\n",
        "1. Retrieve relevant clause (via LlamaIndex)\n",
        "2. Summarize clause\n",
        "3. Extract obligations\n",
        "4. Highlight potential risks\n",
        "5. Provide citation references\n",
        "\n",
        "LangChain enables structured pipelines instead of one-off queries.\n",
        "\n",
        "\n",
        "üîπ Practical Usage Scenario\n",
        "\n",
        "Scenario 1: Contract Review\n",
        "Lawyer uploads 200-page agreement.\n",
        "- System auto-summarizes each section.\n",
        "- Flags risky clauses (e.g., indemnity, liability caps).\n",
        "- Allows natural language querying:\n",
        "  > \"What are the payment terms?\"\n",
        "\n",
        "Scenario 2: Case Research\n",
        "Lawyer asks:\n",
        "> \"Find precedents related to breach of contract in 2021.\"\n",
        "\n",
        "System:\n",
        "- Retrieves similar cases\n",
        "- Summarizes judgments\n",
        "- Extracts legal principles\n",
        "\n",
        "\n",
        "üîπ Advantages of This Approach\n",
        "\n",
        "‚úÖ Speed\n",
        "- Instant retrieval from thousands of documents.\n",
        "\n",
        "‚úÖ Accuracy\n",
        "- Grounded in retrieved legal text (RAG-based).\n",
        "\n",
        "‚úÖ Scalability\n",
        "- Can handle growing document databases.\n",
        "\n",
        "‚úÖ Reduced Manual Effort\n",
        "- Lawyers spend less time reading repetitive clauses.\n",
        "\n",
        "üîπ Ethical and Legal Considerations\n",
        "\n",
        "- Data privacy (confidential case data)\n",
        "- Secure cloud storage\n",
        "- On-premise deployment if required\n",
        "- Audit logging of AI-generated summaries\n",
        "- Human-in-the-loop verification before legal decisions\n",
        "\n",
        "AI should assist‚Äînot replace‚Äîlegal professionals.\n",
        "\n",
        "\n",
        "üîπ Why Use Both LangChain and LlamaIndex?\n",
        "\n",
        "| Component | Role |\n",
        "|------------|------|\n",
        "| LlamaIndex | Efficient indexing & semantic retrieval |\n",
        "| LangChain | Workflow orchestration & reasoning pipelines |\n",
        "\n",
        "They complement each other:\n",
        "- LlamaIndex ‚Üí Handles data\n",
        "- LangChain ‚Üí Handles logic and AI workflows"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c7123874a2cb43538b6d912bff9642e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2974963ef894ddda80da8af2742abec",
              "IPY_MODEL_076aac1d9476474d8a70539dfa133f5f",
              "IPY_MODEL_eafc27a883c74775965f93b9746d2d28"
            ],
            "layout": "IPY_MODEL_c44ae18058d2489598a1354a23a0d6ef"
          }
        },
        "f2974963ef894ddda80da8af2742abec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a28ca8d973b24ed6bea3c8b056afc913",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d1430e604616405da70c3bf289011f8c",
            "value": "Loading‚Äáweights:‚Äá100%"
          }
        },
        "076aac1d9476474d8a70539dfa133f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_092d36b7092d44cb82e26cdb48356e25",
            "max": 199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5281f4f93c264337a75764f52d3bd080",
            "value": 199
          }
        },
        "eafc27a883c74775965f93b9746d2d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7efb89be1b9b46eda99a479cc8ba575e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b18bc16e8e38437f9f28d0fb65a09f74",
            "value": "‚Äá199/199‚Äá[00:00&lt;00:00,‚Äá612.37it/s,‚ÄáMaterializing‚Äáparam=pooler.dense.weight]"
          }
        },
        "c44ae18058d2489598a1354a23a0d6ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a28ca8d973b24ed6bea3c8b056afc913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1430e604616405da70c3bf289011f8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "092d36b7092d44cb82e26cdb48356e25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5281f4f93c264337a75764f52d3bd080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7efb89be1b9b46eda99a479cc8ba575e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b18bc16e8e38437f9f28d0fb65a09f74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}